{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import data_loader\n",
    "import numpy as np\n",
    "import sample_to_chords as s2c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda:0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():  \n",
    "  dev = \"cuda:0\" \n",
    "else:  \n",
    "  dev = \"cpu\"  \n",
    "print('using',dev)\n",
    "device = torch.device(dev)\n",
    "class VAE(nn.Module):\n",
    "    N_CHORDS = 16\n",
    "    N_PITCH = 12\n",
    "    N_QUALITY = 7 # A changer aussi dans data_loader\n",
    "    \n",
    "    SIZE_HIDDEN = 400\n",
    "    SIZE_LATENT = 40\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(self.N_CHORDS * self.N_PITCH * self.N_QUALITY, self.SIZE_HIDDEN)\n",
    "        self.fc21 = nn.Linear(self.SIZE_HIDDEN, self.SIZE_LATENT)\n",
    "        self.fc22 = nn.Linear(self.SIZE_HIDDEN, self.SIZE_LATENT)\n",
    "        \n",
    "        self.fc3 = nn.Linear(self.SIZE_LATENT, self.SIZE_HIDDEN)\n",
    "        self.fc4 = nn.Linear(self.SIZE_HIDDEN, self.N_CHORDS * self.N_PITCH * self.N_QUALITY)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        soft = nn.Sigmoid()\n",
    "        return soft(self.fc4(h3).view(-1, self.N_CHORDS, self.N_PITCH * self.N_QUALITY))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, self.N_CHORDS*self.N_PITCH * self.N_QUALITY))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar, beta):\n",
    "    BCE = F.binary_cross_entropy(recon_x.view(-1, 16*12*7), x.view(-1, 16*12*7), reduction='sum')\n",
    "\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + beta*KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    beta = epoch/epochs\n",
    "    for batch_idx, data in enumerate(realbook_dataset):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    \n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar, beta)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), Nchunks,\n",
    "                100. * batch_idx * len(data)/ Nchunks,\n",
    "                loss.item() / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / Nchunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 128\n",
    "log_interval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded !\n"
     ]
    }
   ],
   "source": [
    "realbook_dataset = data_loader.import_dataset()\n",
    "Nchunks = len(realbook_dataset)\n",
    "realbook_dataset = torch.split(realbook_dataset, batch_size, 0)\n",
    "model = VAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"./model_realbook.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/394813 (0%)]\tLoss: 19.509785\n",
      "Train Epoch: 1 [12800/394813 (3%)]\tLoss: 18.713629\n",
      "Train Epoch: 1 [25600/394813 (6%)]\tLoss: 2.831147\n",
      "Train Epoch: 1 [38400/394813 (10%)]\tLoss: 8.213997\n",
      "Train Epoch: 1 [51200/394813 (13%)]\tLoss: 10.953599\n",
      "Train Epoch: 1 [64000/394813 (16%)]\tLoss: 10.448996\n",
      "Train Epoch: 1 [76800/394813 (19%)]\tLoss: 7.194133\n",
      "Train Epoch: 1 [89600/394813 (23%)]\tLoss: 15.511894\n",
      "Train Epoch: 1 [102400/394813 (26%)]\tLoss: 10.580265\n",
      "Train Epoch: 1 [115200/394813 (29%)]\tLoss: 7.022242\n",
      "Train Epoch: 1 [128000/394813 (32%)]\tLoss: 5.701787\n",
      "Train Epoch: 1 [140800/394813 (36%)]\tLoss: 10.628595\n",
      "Train Epoch: 1 [153600/394813 (39%)]\tLoss: 5.285030\n",
      "Train Epoch: 1 [166400/394813 (42%)]\tLoss: 11.568071\n",
      "Train Epoch: 1 [179200/394813 (45%)]\tLoss: 8.946575\n",
      "Train Epoch: 1 [192000/394813 (49%)]\tLoss: 15.590571\n",
      "Train Epoch: 1 [204800/394813 (52%)]\tLoss: 8.564662\n",
      "Train Epoch: 1 [217600/394813 (55%)]\tLoss: 17.871433\n",
      "Train Epoch: 1 [230400/394813 (58%)]\tLoss: 11.772414\n",
      "Train Epoch: 1 [243200/394813 (62%)]\tLoss: 27.518324\n",
      "Train Epoch: 1 [256000/394813 (65%)]\tLoss: 9.577600\n",
      "Train Epoch: 1 [268800/394813 (68%)]\tLoss: 10.807207\n",
      "Train Epoch: 1 [281600/394813 (71%)]\tLoss: 8.154900\n",
      "Train Epoch: 1 [294400/394813 (75%)]\tLoss: 15.611446\n",
      "Train Epoch: 1 [307200/394813 (78%)]\tLoss: 3.368857\n",
      "Train Epoch: 1 [320000/394813 (81%)]\tLoss: 12.159664\n",
      "Train Epoch: 1 [332800/394813 (84%)]\tLoss: 8.183775\n",
      "Train Epoch: 1 [345600/394813 (88%)]\tLoss: 11.509231\n",
      "Train Epoch: 1 [358400/394813 (91%)]\tLoss: 15.509995\n",
      "Train Epoch: 1 [371200/394813 (94%)]\tLoss: 10.822639\n",
      "Train Epoch: 1 [384000/394813 (97%)]\tLoss: 1.520375\n",
      "====> Epoch: 1 Average loss: 11.8307\n",
      "Train Epoch: 2 [0/394813 (0%)]\tLoss: 15.435221\n",
      "Train Epoch: 2 [12800/394813 (3%)]\tLoss: 21.864399\n",
      "Train Epoch: 2 [25600/394813 (6%)]\tLoss: 4.916973\n",
      "Train Epoch: 2 [38400/394813 (10%)]\tLoss: 11.480001\n",
      "Train Epoch: 2 [51200/394813 (13%)]\tLoss: 14.540070\n",
      "Train Epoch: 2 [64000/394813 (16%)]\tLoss: 13.419411\n",
      "Train Epoch: 2 [76800/394813 (19%)]\tLoss: 10.303853\n",
      "Train Epoch: 2 [89600/394813 (23%)]\tLoss: 19.458485\n",
      "Train Epoch: 2 [102400/394813 (26%)]\tLoss: 13.809947\n",
      "Train Epoch: 2 [115200/394813 (29%)]\tLoss: 10.392817\n",
      "Train Epoch: 2 [128000/394813 (32%)]\tLoss: 8.878683\n",
      "Train Epoch: 2 [140800/394813 (36%)]\tLoss: 14.293747\n",
      "Train Epoch: 2 [153600/394813 (39%)]\tLoss: 7.900891\n",
      "Train Epoch: 2 [166400/394813 (42%)]\tLoss: 15.409242\n",
      "Train Epoch: 2 [179200/394813 (45%)]\tLoss: 12.936735\n",
      "Train Epoch: 2 [192000/394813 (49%)]\tLoss: 19.909836\n",
      "Train Epoch: 2 [204800/394813 (52%)]\tLoss: 11.327021\n",
      "Train Epoch: 2 [217600/394813 (55%)]\tLoss: 21.483955\n",
      "Train Epoch: 2 [230400/394813 (58%)]\tLoss: 15.333619\n",
      "Train Epoch: 2 [243200/394813 (62%)]\tLoss: 31.786709\n",
      "Train Epoch: 2 [256000/394813 (65%)]\tLoss: 13.187126\n",
      "Train Epoch: 2 [268800/394813 (68%)]\tLoss: 15.400990\n",
      "Train Epoch: 2 [281600/394813 (71%)]\tLoss: 11.544415\n",
      "Train Epoch: 2 [294400/394813 (75%)]\tLoss: 20.595898\n",
      "Train Epoch: 2 [307200/394813 (78%)]\tLoss: 5.885156\n",
      "Train Epoch: 2 [320000/394813 (81%)]\tLoss: 16.326416\n",
      "Train Epoch: 2 [332800/394813 (84%)]\tLoss: 11.734364\n",
      "Train Epoch: 2 [345600/394813 (88%)]\tLoss: 16.102802\n",
      "Train Epoch: 2 [358400/394813 (91%)]\tLoss: 19.910906\n",
      "Train Epoch: 2 [371200/394813 (94%)]\tLoss: 15.012114\n",
      "Train Epoch: 2 [384000/394813 (97%)]\tLoss: 2.497306\n",
      "====> Epoch: 2 Average loss: 15.4519\n",
      "Train Epoch: 3 [0/394813 (0%)]\tLoss: 19.229645\n",
      "Train Epoch: 3 [12800/394813 (3%)]\tLoss: 25.615330\n",
      "Train Epoch: 3 [25600/394813 (6%)]\tLoss: 6.226004\n",
      "Train Epoch: 3 [38400/394813 (10%)]\tLoss: 15.043011\n",
      "Train Epoch: 3 [51200/394813 (13%)]\tLoss: 18.400908\n",
      "Train Epoch: 3 [64000/394813 (16%)]\tLoss: 16.691437\n",
      "Train Epoch: 3 [76800/394813 (19%)]\tLoss: 13.557858\n",
      "Train Epoch: 3 [89600/394813 (23%)]\tLoss: 23.288023\n",
      "Train Epoch: 3 [102400/394813 (26%)]\tLoss: 18.211998\n",
      "Train Epoch: 3 [115200/394813 (29%)]\tLoss: 12.478400\n",
      "Train Epoch: 3 [128000/394813 (32%)]\tLoss: 11.438593\n",
      "Train Epoch: 3 [140800/394813 (36%)]\tLoss: 18.255264\n",
      "Train Epoch: 3 [153600/394813 (39%)]\tLoss: 10.492185\n",
      "Train Epoch: 3 [166400/394813 (42%)]\tLoss: 19.748220\n",
      "Train Epoch: 3 [179200/394813 (45%)]\tLoss: 16.802908\n",
      "Train Epoch: 3 [192000/394813 (49%)]\tLoss: 24.028599\n",
      "Train Epoch: 3 [204800/394813 (52%)]\tLoss: 13.612513\n",
      "Train Epoch: 3 [217600/394813 (55%)]\tLoss: 25.915310\n",
      "Train Epoch: 3 [230400/394813 (58%)]\tLoss: 18.730927\n",
      "Train Epoch: 3 [243200/394813 (62%)]\tLoss: 35.776276\n",
      "Train Epoch: 3 [256000/394813 (65%)]\tLoss: 19.064739\n",
      "Train Epoch: 3 [268800/394813 (68%)]\tLoss: 19.312328\n",
      "Train Epoch: 3 [281600/394813 (71%)]\tLoss: 15.014335\n",
      "Train Epoch: 3 [294400/394813 (75%)]\tLoss: 24.706724\n",
      "Train Epoch: 3 [307200/394813 (78%)]\tLoss: 7.759435\n",
      "Train Epoch: 3 [320000/394813 (81%)]\tLoss: 20.417759\n",
      "Train Epoch: 3 [332800/394813 (84%)]\tLoss: 14.991652\n",
      "Train Epoch: 3 [345600/394813 (88%)]\tLoss: 20.616028\n",
      "Train Epoch: 3 [358400/394813 (91%)]\tLoss: 23.725731\n",
      "Train Epoch: 3 [371200/394813 (94%)]\tLoss: 19.228258\n",
      "Train Epoch: 3 [384000/394813 (97%)]\tLoss: 3.487352\n",
      "====> Epoch: 3 Average loss: 19.0528\n",
      "Train Epoch: 4 [0/394813 (0%)]\tLoss: 22.266527\n",
      "Train Epoch: 4 [12800/394813 (3%)]\tLoss: 28.693390\n",
      "Train Epoch: 4 [25600/394813 (6%)]\tLoss: 8.126314\n",
      "Train Epoch: 4 [38400/394813 (10%)]\tLoss: 17.751419\n",
      "Train Epoch: 4 [51200/394813 (13%)]\tLoss: 21.695370\n",
      "Train Epoch: 4 [64000/394813 (16%)]\tLoss: 19.840912\n",
      "Train Epoch: 4 [76800/394813 (19%)]\tLoss: 15.465812\n",
      "Train Epoch: 4 [89600/394813 (23%)]\tLoss: 27.158985\n",
      "Train Epoch: 4 [102400/394813 (26%)]\tLoss: 21.425432\n",
      "Train Epoch: 4 [115200/394813 (29%)]\tLoss: 15.327652\n",
      "Train Epoch: 4 [128000/394813 (32%)]\tLoss: 14.571723\n",
      "Train Epoch: 4 [140800/394813 (36%)]\tLoss: 21.297070\n",
      "Train Epoch: 4 [153600/394813 (39%)]\tLoss: 12.504061\n",
      "Train Epoch: 4 [166400/394813 (42%)]\tLoss: 23.582649\n",
      "Train Epoch: 4 [179200/394813 (45%)]\tLoss: 19.859097\n",
      "Train Epoch: 4 [192000/394813 (49%)]\tLoss: 27.940954\n",
      "Train Epoch: 4 [204800/394813 (52%)]\tLoss: 16.181974\n",
      "Train Epoch: 4 [217600/394813 (55%)]\tLoss: 29.746048\n",
      "Train Epoch: 4 [230400/394813 (58%)]\tLoss: 21.594133\n",
      "Train Epoch: 4 [243200/394813 (62%)]\tLoss: 38.932781\n",
      "Train Epoch: 4 [256000/394813 (65%)]\tLoss: 20.614170\n",
      "Train Epoch: 4 [268800/394813 (68%)]\tLoss: 22.992867\n",
      "Train Epoch: 4 [281600/394813 (71%)]\tLoss: 17.940750\n",
      "Train Epoch: 4 [294400/394813 (75%)]\tLoss: 29.822727\n",
      "Train Epoch: 4 [307200/394813 (78%)]\tLoss: 9.571802\n",
      "Train Epoch: 4 [320000/394813 (81%)]\tLoss: 24.737392\n",
      "Train Epoch: 4 [332800/394813 (84%)]\tLoss: 18.518412\n",
      "Train Epoch: 4 [345600/394813 (88%)]\tLoss: 23.851757\n",
      "Train Epoch: 4 [358400/394813 (91%)]\tLoss: 28.057888\n",
      "Train Epoch: 4 [371200/394813 (94%)]\tLoss: 22.794548\n",
      "Train Epoch: 4 [384000/394813 (97%)]\tLoss: 4.045928\n",
      "====> Epoch: 4 Average loss: 22.3001\n",
      "Train Epoch: 5 [0/394813 (0%)]\tLoss: 25.372194\n",
      "Train Epoch: 5 [12800/394813 (3%)]\tLoss: 33.630695\n",
      "Train Epoch: 5 [25600/394813 (6%)]\tLoss: 9.486524\n",
      "Train Epoch: 5 [38400/394813 (10%)]\tLoss: 20.706156\n",
      "Train Epoch: 5 [51200/394813 (13%)]\tLoss: 24.493813\n",
      "Train Epoch: 5 [64000/394813 (16%)]\tLoss: 22.134436\n",
      "Train Epoch: 5 [76800/394813 (19%)]\tLoss: 19.008963\n",
      "Train Epoch: 5 [89600/394813 (23%)]\tLoss: 31.038410\n",
      "Train Epoch: 5 [102400/394813 (26%)]\tLoss: 24.093796\n",
      "Train Epoch: 5 [115200/394813 (29%)]\tLoss: 17.736580\n",
      "Train Epoch: 5 [128000/394813 (32%)]\tLoss: 17.003790\n",
      "Train Epoch: 5 [140800/394813 (36%)]\tLoss: 24.590265\n",
      "Train Epoch: 5 [153600/394813 (39%)]\tLoss: 14.316740\n",
      "Train Epoch: 5 [166400/394813 (42%)]\tLoss: 26.610477\n",
      "Train Epoch: 5 [179200/394813 (45%)]\tLoss: 22.504818\n",
      "Train Epoch: 5 [192000/394813 (49%)]\tLoss: 32.026691\n",
      "Train Epoch: 5 [204800/394813 (52%)]\tLoss: 18.058758\n",
      "Train Epoch: 5 [217600/394813 (55%)]\tLoss: 33.445412\n",
      "Train Epoch: 5 [230400/394813 (58%)]\tLoss: 23.246666\n",
      "Train Epoch: 5 [243200/394813 (62%)]\tLoss: 43.787777\n",
      "Train Epoch: 5 [256000/394813 (65%)]\tLoss: 22.033535\n",
      "Train Epoch: 5 [268800/394813 (68%)]\tLoss: 27.075420\n",
      "Train Epoch: 5 [281600/394813 (71%)]\tLoss: 20.276169\n",
      "Train Epoch: 5 [294400/394813 (75%)]\tLoss: 32.708305\n",
      "Train Epoch: 5 [307200/394813 (78%)]\tLoss: 11.518963\n",
      "Train Epoch: 5 [320000/394813 (81%)]\tLoss: 28.357697\n",
      "Train Epoch: 5 [332800/394813 (84%)]\tLoss: 20.589937\n",
      "Train Epoch: 5 [345600/394813 (88%)]\tLoss: 27.101006\n",
      "Train Epoch: 5 [358400/394813 (91%)]\tLoss: 30.932888\n",
      "Train Epoch: 5 [371200/394813 (94%)]\tLoss: 25.828716\n",
      "Train Epoch: 5 [384000/394813 (97%)]\tLoss: 4.894433\n",
      "====> Epoch: 5 Average loss: 25.2961\n",
      "Train Epoch: 6 [0/394813 (0%)]\tLoss: 28.395748\n",
      "Train Epoch: 6 [12800/394813 (3%)]\tLoss: 35.990463\n",
      "Train Epoch: 6 [25600/394813 (6%)]\tLoss: 10.551903\n",
      "Train Epoch: 6 [38400/394813 (10%)]\tLoss: 23.247852\n",
      "Train Epoch: 6 [51200/394813 (13%)]\tLoss: 27.556717\n",
      "Train Epoch: 6 [64000/394813 (16%)]\tLoss: 25.028568\n",
      "Train Epoch: 6 [76800/394813 (19%)]\tLoss: 20.787533\n",
      "Train Epoch: 6 [89600/394813 (23%)]\tLoss: 33.299995\n",
      "Train Epoch: 6 [102400/394813 (26%)]\tLoss: 26.922596\n",
      "Train Epoch: 6 [115200/394813 (29%)]\tLoss: 19.510691\n",
      "Train Epoch: 6 [128000/394813 (32%)]\tLoss: 19.577360\n",
      "Train Epoch: 6 [140800/394813 (36%)]\tLoss: 27.279503\n",
      "Train Epoch: 6 [153600/394813 (39%)]\tLoss: 16.261093\n",
      "Train Epoch: 6 [166400/394813 (42%)]\tLoss: 29.477900\n",
      "Train Epoch: 6 [179200/394813 (45%)]\tLoss: 25.644894\n",
      "Train Epoch: 6 [192000/394813 (49%)]\tLoss: 35.551926\n",
      "Train Epoch: 6 [204800/394813 (52%)]\tLoss: 19.497772\n",
      "Train Epoch: 6 [217600/394813 (55%)]\tLoss: 37.122860\n",
      "Train Epoch: 6 [230400/394813 (58%)]\tLoss: 26.252914\n",
      "Train Epoch: 6 [243200/394813 (62%)]\tLoss: 46.833199\n",
      "Train Epoch: 6 [256000/394813 (65%)]\tLoss: 25.415703\n",
      "Train Epoch: 6 [268800/394813 (68%)]\tLoss: 29.893677\n",
      "Train Epoch: 6 [281600/394813 (71%)]\tLoss: 22.637901\n",
      "Train Epoch: 6 [294400/394813 (75%)]\tLoss: 36.661022\n",
      "Train Epoch: 6 [307200/394813 (78%)]\tLoss: 12.554035\n",
      "Train Epoch: 6 [320000/394813 (81%)]\tLoss: 32.139244\n",
      "Train Epoch: 6 [332800/394813 (84%)]\tLoss: 23.476753\n",
      "Train Epoch: 6 [345600/394813 (88%)]\tLoss: 30.666246\n",
      "Train Epoch: 6 [358400/394813 (91%)]\tLoss: 34.206146\n",
      "Train Epoch: 6 [371200/394813 (94%)]\tLoss: 29.691219\n",
      "Train Epoch: 6 [384000/394813 (97%)]\tLoss: 5.997247\n",
      "====> Epoch: 6 Average loss: 28.0334\n",
      "Train Epoch: 7 [0/394813 (0%)]\tLoss: 30.275194\n",
      "Train Epoch: 7 [12800/394813 (3%)]\tLoss: 39.796112\n",
      "Train Epoch: 7 [25600/394813 (6%)]\tLoss: 11.624864\n",
      "Train Epoch: 7 [38400/394813 (10%)]\tLoss: 26.137457\n",
      "Train Epoch: 7 [51200/394813 (13%)]\tLoss: 30.849342\n",
      "Train Epoch: 7 [64000/394813 (16%)]\tLoss: 26.737335\n",
      "Train Epoch: 7 [76800/394813 (19%)]\tLoss: 23.152149\n",
      "Train Epoch: 7 [89600/394813 (23%)]\tLoss: 36.441284\n",
      "Train Epoch: 7 [102400/394813 (26%)]\tLoss: 29.757412\n",
      "Train Epoch: 7 [115200/394813 (29%)]\tLoss: 21.791578\n",
      "Train Epoch: 7 [128000/394813 (32%)]\tLoss: 22.113239\n",
      "Train Epoch: 7 [140800/394813 (36%)]\tLoss: 30.510061\n",
      "Train Epoch: 7 [153600/394813 (39%)]\tLoss: 18.679344\n",
      "Train Epoch: 7 [166400/394813 (42%)]\tLoss: 31.723352\n",
      "Train Epoch: 7 [179200/394813 (45%)]\tLoss: 27.945358\n",
      "Train Epoch: 7 [192000/394813 (49%)]\tLoss: 37.912495\n",
      "Train Epoch: 7 [204800/394813 (52%)]\tLoss: 21.789570\n",
      "Train Epoch: 7 [217600/394813 (55%)]\tLoss: 39.467552\n",
      "Train Epoch: 7 [230400/394813 (58%)]\tLoss: 28.463058\n",
      "Train Epoch: 7 [243200/394813 (62%)]\tLoss: 49.086914\n",
      "Train Epoch: 7 [256000/394813 (65%)]\tLoss: 27.655104\n",
      "Train Epoch: 7 [268800/394813 (68%)]\tLoss: 32.485397\n",
      "Train Epoch: 7 [281600/394813 (71%)]\tLoss: 24.088814\n",
      "Train Epoch: 7 [294400/394813 (75%)]\tLoss: 40.731071\n",
      "Train Epoch: 7 [307200/394813 (78%)]\tLoss: 14.122166\n",
      "Train Epoch: 7 [320000/394813 (81%)]\tLoss: 34.971481\n",
      "Train Epoch: 7 [332800/394813 (84%)]\tLoss: 25.053211\n",
      "Train Epoch: 7 [345600/394813 (88%)]\tLoss: 33.060085\n",
      "Train Epoch: 7 [358400/394813 (91%)]\tLoss: 37.466927\n",
      "Train Epoch: 7 [371200/394813 (94%)]\tLoss: 32.427143\n",
      "Train Epoch: 7 [384000/394813 (97%)]\tLoss: 6.082815\n",
      "====> Epoch: 7 Average loss: 30.5383\n",
      "Train Epoch: 8 [0/394813 (0%)]\tLoss: 33.063030\n",
      "Train Epoch: 8 [12800/394813 (3%)]\tLoss: 42.462681\n",
      "Train Epoch: 8 [25600/394813 (6%)]\tLoss: 13.090021\n",
      "Train Epoch: 8 [38400/394813 (10%)]\tLoss: 28.257860\n",
      "Train Epoch: 8 [51200/394813 (13%)]\tLoss: 32.651398\n",
      "Train Epoch: 8 [64000/394813 (16%)]\tLoss: 29.850796\n",
      "Train Epoch: 8 [76800/394813 (19%)]\tLoss: 25.675392\n",
      "Train Epoch: 8 [89600/394813 (23%)]\tLoss: 38.665577\n",
      "Train Epoch: 8 [102400/394813 (26%)]\tLoss: 32.436272\n",
      "Train Epoch: 8 [115200/394813 (29%)]\tLoss: 23.652948\n",
      "Train Epoch: 8 [128000/394813 (32%)]\tLoss: 23.058231\n",
      "Train Epoch: 8 [140800/394813 (36%)]\tLoss: 31.927143\n",
      "Train Epoch: 8 [153600/394813 (39%)]\tLoss: 19.739420\n",
      "Train Epoch: 8 [166400/394813 (42%)]\tLoss: 35.328896\n",
      "Train Epoch: 8 [179200/394813 (45%)]\tLoss: 30.543280\n",
      "Train Epoch: 8 [192000/394813 (49%)]\tLoss: 40.653366\n",
      "Train Epoch: 8 [204800/394813 (52%)]\tLoss: 23.382885\n",
      "Train Epoch: 8 [217600/394813 (55%)]\tLoss: 43.311199\n",
      "Train Epoch: 8 [230400/394813 (58%)]\tLoss: 29.516884\n",
      "Train Epoch: 8 [243200/394813 (62%)]\tLoss: 52.369026\n",
      "Train Epoch: 8 [256000/394813 (65%)]\tLoss: 29.848969\n",
      "Train Epoch: 8 [268800/394813 (68%)]\tLoss: 35.581253\n",
      "Train Epoch: 8 [281600/394813 (71%)]\tLoss: 26.707054\n",
      "Train Epoch: 8 [294400/394813 (75%)]\tLoss: 43.873383\n",
      "Train Epoch: 8 [307200/394813 (78%)]\tLoss: 15.164423\n",
      "Train Epoch: 8 [320000/394813 (81%)]\tLoss: 37.793964\n",
      "Train Epoch: 8 [332800/394813 (84%)]\tLoss: 26.822628\n",
      "Train Epoch: 8 [345600/394813 (88%)]\tLoss: 35.486755\n",
      "Train Epoch: 8 [358400/394813 (91%)]\tLoss: 39.300941\n",
      "Train Epoch: 8 [371200/394813 (94%)]\tLoss: 35.050350\n",
      "Train Epoch: 8 [384000/394813 (97%)]\tLoss: 6.686283\n",
      "====> Epoch: 8 Average loss: 32.8766\n",
      "Train Epoch: 9 [0/394813 (0%)]\tLoss: 36.491833\n",
      "Train Epoch: 9 [12800/394813 (3%)]\tLoss: 44.597557\n",
      "Train Epoch: 9 [25600/394813 (6%)]\tLoss: 13.634295\n",
      "Train Epoch: 9 [38400/394813 (10%)]\tLoss: 29.897385\n",
      "Train Epoch: 9 [51200/394813 (13%)]\tLoss: 35.335281\n",
      "Train Epoch: 9 [64000/394813 (16%)]\tLoss: 30.688332\n",
      "Train Epoch: 9 [76800/394813 (19%)]\tLoss: 26.513714\n",
      "Train Epoch: 9 [89600/394813 (23%)]\tLoss: 41.359692\n",
      "Train Epoch: 9 [102400/394813 (26%)]\tLoss: 34.481544\n",
      "Train Epoch: 9 [115200/394813 (29%)]\tLoss: 24.825684\n",
      "Train Epoch: 9 [128000/394813 (32%)]\tLoss: 24.872429\n",
      "Train Epoch: 9 [140800/394813 (36%)]\tLoss: 34.916847\n",
      "Train Epoch: 9 [153600/394813 (39%)]\tLoss: 21.178808\n",
      "Train Epoch: 9 [166400/394813 (42%)]\tLoss: 37.426659\n",
      "Train Epoch: 9 [179200/394813 (45%)]\tLoss: 33.003078\n",
      "Train Epoch: 9 [192000/394813 (49%)]\tLoss: 43.866802\n",
      "Train Epoch: 9 [204800/394813 (52%)]\tLoss: 25.080482\n",
      "Train Epoch: 9 [217600/394813 (55%)]\tLoss: 45.141949\n",
      "Train Epoch: 9 [230400/394813 (58%)]\tLoss: 32.035904\n",
      "Train Epoch: 9 [243200/394813 (62%)]\tLoss: 54.608452\n",
      "Train Epoch: 9 [256000/394813 (65%)]\tLoss: 31.037939\n",
      "Train Epoch: 9 [268800/394813 (68%)]\tLoss: 38.299648\n",
      "Train Epoch: 9 [281600/394813 (71%)]\tLoss: 28.914797\n",
      "Train Epoch: 9 [294400/394813 (75%)]\tLoss: 46.367825\n",
      "Train Epoch: 9 [307200/394813 (78%)]\tLoss: 15.888115\n",
      "Train Epoch: 9 [320000/394813 (81%)]\tLoss: 40.709827\n",
      "Train Epoch: 9 [332800/394813 (84%)]\tLoss: 29.229708\n",
      "Train Epoch: 9 [345600/394813 (88%)]\tLoss: 38.223621\n",
      "Train Epoch: 9 [358400/394813 (91%)]\tLoss: 42.464447\n",
      "Train Epoch: 9 [371200/394813 (94%)]\tLoss: 37.390865\n",
      "Train Epoch: 9 [384000/394813 (97%)]\tLoss: 7.761161\n",
      "====> Epoch: 9 Average loss: 35.0416\n",
      "Train Epoch: 10 [0/394813 (0%)]\tLoss: 38.261406\n",
      "Train Epoch: 10 [12800/394813 (3%)]\tLoss: 46.262840\n",
      "Train Epoch: 10 [25600/394813 (6%)]\tLoss: 14.506052\n",
      "Train Epoch: 10 [38400/394813 (10%)]\tLoss: 32.012611\n",
      "Train Epoch: 10 [51200/394813 (13%)]\tLoss: 36.821274\n",
      "Train Epoch: 10 [64000/394813 (16%)]\tLoss: 32.503124\n",
      "Train Epoch: 10 [76800/394813 (19%)]\tLoss: 27.855127\n",
      "Train Epoch: 10 [89600/394813 (23%)]\tLoss: 44.854538\n",
      "Train Epoch: 10 [102400/394813 (26%)]\tLoss: 36.506927\n",
      "Train Epoch: 10 [115200/394813 (29%)]\tLoss: 26.349274\n",
      "Train Epoch: 10 [128000/394813 (32%)]\tLoss: 26.775501\n",
      "Train Epoch: 10 [140800/394813 (36%)]\tLoss: 36.566643\n",
      "Train Epoch: 10 [153600/394813 (39%)]\tLoss: 23.010935\n",
      "Train Epoch: 10 [166400/394813 (42%)]\tLoss: 39.469887\n",
      "Train Epoch: 10 [179200/394813 (45%)]\tLoss: 34.882576\n",
      "Train Epoch: 10 [192000/394813 (49%)]\tLoss: 46.459869\n",
      "Train Epoch: 10 [204800/394813 (52%)]\tLoss: 26.159618\n",
      "Train Epoch: 10 [217600/394813 (55%)]\tLoss: 47.600594\n",
      "Train Epoch: 10 [230400/394813 (58%)]\tLoss: 34.042534\n",
      "Train Epoch: 10 [243200/394813 (62%)]\tLoss: 57.189758\n",
      "Train Epoch: 10 [256000/394813 (65%)]\tLoss: 32.674324\n",
      "Train Epoch: 10 [268800/394813 (68%)]\tLoss: 40.739357\n",
      "Train Epoch: 10 [281600/394813 (71%)]\tLoss: 30.518677\n",
      "Train Epoch: 10 [294400/394813 (75%)]\tLoss: 48.340126\n",
      "Train Epoch: 10 [307200/394813 (78%)]\tLoss: 17.747255\n",
      "Train Epoch: 10 [320000/394813 (81%)]\tLoss: 43.713158\n",
      "Train Epoch: 10 [332800/394813 (84%)]\tLoss: 31.249062\n",
      "Train Epoch: 10 [345600/394813 (88%)]\tLoss: 39.628437\n",
      "Train Epoch: 10 [358400/394813 (91%)]\tLoss: 44.675537\n",
      "Train Epoch: 10 [371200/394813 (94%)]\tLoss: 37.902542\n",
      "Train Epoch: 10 [384000/394813 (97%)]\tLoss: 8.091269\n",
      "====> Epoch: 10 Average loss: 37.0153\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./model_realbook.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vérité\n",
      "['A:min7', 'A:min7', 'A:min7', 'A:min7', 'E:min7', 'E:min7', 'E:min7', 'E:min7', 'E:min7', 'E:min7', 'E:min7', 'E:min7', 'F:maj7', 'F:maj7', 'F:maj7', 'F:maj7']\n",
      "\n",
      "Par VAE\n",
      "['A:min7', 'A:min7', 'A:min7', 'A:min7', 'E:min7', 'E:min7', 'E:min7', 'E:min7', 'E:min7', 'E:min7', 'E:min7', 'E:min7', 'F:maj7', 'F:maj7', 'F:maj7', 'F:maj7']\n"
     ]
    }
   ],
   "source": [
    "PITCH_LIST = [\"A\", \"A#\", \"B\", \"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\"]\n",
    "QUALITY_LIST = [\"maj\", \"min\", \"dim\", \"maj7\", \"min7\", \"7\", \"dim7\"]\n",
    "\n",
    "def sample_to_chords(sample):\n",
    "    idx_chords = np.argmax(sample[0,:,:],1)\n",
    "#     print(idx_chords)\n",
    "    chords = [PITCH_LIST[int(idx/7)] + \":\" + QUALITY_LIST[int(idx%7)] for idx in idx_chords]\n",
    "    return chords\n",
    "\n",
    "index_test = 68\n",
    "test_sample = realbook_dataset[0][index_test]\n",
    "\n",
    "print(\"Vérité\")\n",
    "true_sample = test_sample.view(1, 16, -1).numpy()\n",
    "print(sample_to_chords(true_sample))\n",
    "\n",
    "print()\n",
    "print(\"Par VAE\")\n",
    "model.to(torch.device(\"cpu\"))\n",
    "recons_test, _, _ = model(test_sample)\n",
    "print(sample_to_chords(recons_test.detach().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 16, 84)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['D#:maj',\n",
       " 'A:7',\n",
       " 'A:7',\n",
       " 'A:7',\n",
       " 'A:7',\n",
       " 'A:maj',\n",
       " 'D:min7',\n",
       " 'D:min7',\n",
       " 'A:min7',\n",
       " 'D:7',\n",
       " 'D:7',\n",
       " 'D:7',\n",
       " 'D:7',\n",
       " 'D:7',\n",
       " 'G:min7',\n",
       " 'C:maj']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_LATENT = 40\n",
    "with torch.no_grad():\n",
    "    sample = torch.randn(1, N_LATENT)\n",
    "    sample = model.decode(sample).cpu()\n",
    "    sample = sample.numpy()\n",
    "print(sample.shape)\n",
    "sample_to_chords(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3085"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
