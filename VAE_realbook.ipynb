{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import data_loader\n",
    "import numpy as np\n",
    "import sample_to_chords as s2c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda:0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():  \n",
    "  dev = \"cuda:0\" \n",
    "else:  \n",
    "  dev = \"cpu\"  \n",
    "print('using',dev)\n",
    "device = torch.device(dev)\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        N_HIDDEN = 400\n",
    "        N_LATENT = 40\n",
    "        \n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(16*12*7, N_HIDDEN)\n",
    "        self.fc21 = nn.Linear(N_HIDDEN, N_LATENT)\n",
    "        self.fc22 = nn.Linear(N_HIDDEN, N_LATENT)\n",
    "        self.fc3 = nn.Linear(N_LATENT, N_HIDDEN)\n",
    "        self.fc4 = nn.Linear(N_HIDDEN, 16*12*7)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        soft = nn.Sigmoid()\n",
    "        return soft(self.fc4(h3).view(-1, 16, 12*7))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 16*12*7))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar, beta):\n",
    "    BCE = F.binary_cross_entropy(recon_x.view(-1, 16*12*7), x.view(-1, 16*12*7), reduction='sum')\n",
    "\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + beta*KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    beta = epoch/epochs\n",
    "    for batch_idx, data in enumerate(realbook_dataset):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    \n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar, beta)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), Nchunks,\n",
    "                100. * batch_idx * len(data)/ Nchunks,\n",
    "                loss.item() / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / Nchunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 128\n",
    "log_interval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset not found.\n",
      "Computing dataset...\n",
      "..................................................\n",
      "corpus length: 3531261\n",
      "total diff chords: 1260\n",
      "total number of sentences :  2846\n",
      "..................................................\n",
      "total number of chunks :  494073\n",
      "total number of accepted chunks :  394813  (ratio :  0.8 )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/394813 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting into tensor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 394813/394813 [01:38<00:00, 3988.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded !\n"
     ]
    }
   ],
   "source": [
    "realbook_dataset = data_loader.import_dataset()\n",
    "Nchunks = len(realbook_dataset)\n",
    "realbook_dataset = torch.split(realbook_dataset, batch_size, 0)\n",
    "model = VAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './model_realbook.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-edaba6511e30>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./model_realbook.pt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    523\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoding'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 525\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    526\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    213\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'w'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './model_realbook.pt'"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"./model_realbook.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/394813 (0%)]\tLoss: 942.113342\n",
      "Train Epoch: 1 [12800/394813 (3%)]\tLoss: 94.098923\n",
      "Train Epoch: 1 [25600/394813 (6%)]\tLoss: 34.298676\n",
      "Train Epoch: 1 [38400/394813 (10%)]\tLoss: 58.898575\n",
      "Train Epoch: 1 [51200/394813 (13%)]\tLoss: 61.682579\n",
      "Train Epoch: 1 [64000/394813 (16%)]\tLoss: 53.120510\n",
      "Train Epoch: 1 [76800/394813 (19%)]\tLoss: 41.869110\n",
      "Train Epoch: 1 [89600/394813 (23%)]\tLoss: 57.704853\n",
      "Train Epoch: 1 [102400/394813 (26%)]\tLoss: 45.584061\n",
      "Train Epoch: 1 [115200/394813 (29%)]\tLoss: 26.427004\n",
      "Train Epoch: 1 [128000/394813 (32%)]\tLoss: 24.497372\n",
      "Train Epoch: 1 [140800/394813 (36%)]\tLoss: 42.108967\n",
      "Train Epoch: 1 [153600/394813 (39%)]\tLoss: 20.504969\n",
      "Train Epoch: 1 [166400/394813 (42%)]\tLoss: 36.277988\n",
      "Train Epoch: 1 [179200/394813 (45%)]\tLoss: 28.770489\n",
      "Train Epoch: 1 [192000/394813 (49%)]\tLoss: 45.150951\n",
      "Train Epoch: 1 [204800/394813 (52%)]\tLoss: 17.747032\n",
      "Train Epoch: 1 [217600/394813 (55%)]\tLoss: 39.131351\n",
      "Train Epoch: 1 [230400/394813 (58%)]\tLoss: 21.851477\n",
      "Train Epoch: 1 [243200/394813 (62%)]\tLoss: 49.549019\n",
      "Train Epoch: 1 [256000/394813 (65%)]\tLoss: 24.189857\n",
      "Train Epoch: 1 [268800/394813 (68%)]\tLoss: 26.923996\n",
      "Train Epoch: 1 [281600/394813 (71%)]\tLoss: 18.967279\n",
      "Train Epoch: 1 [294400/394813 (75%)]\tLoss: 39.006859\n",
      "Train Epoch: 1 [307200/394813 (78%)]\tLoss: 7.327650\n",
      "Train Epoch: 1 [320000/394813 (81%)]\tLoss: 47.422474\n",
      "Train Epoch: 1 [332800/394813 (84%)]\tLoss: 15.937826\n",
      "Train Epoch: 1 [345600/394813 (88%)]\tLoss: 24.733242\n",
      "Train Epoch: 1 [358400/394813 (91%)]\tLoss: 31.475388\n",
      "Train Epoch: 1 [371200/394813 (94%)]\tLoss: 24.348349\n",
      "Train Epoch: 1 [384000/394813 (97%)]\tLoss: 2.777231\n",
      "====> Epoch: 1 Average loss: 40.3411\n",
      "Train Epoch: 2 [0/394813 (0%)]\tLoss: 26.913164\n",
      "Train Epoch: 2 [12800/394813 (3%)]\tLoss: 39.587807\n",
      "Train Epoch: 2 [25600/394813 (6%)]\tLoss: 7.371362\n",
      "Train Epoch: 2 [38400/394813 (10%)]\tLoss: 18.894169\n",
      "Train Epoch: 2 [51200/394813 (13%)]\tLoss: 24.319117\n",
      "Train Epoch: 2 [64000/394813 (16%)]\tLoss: 21.704866\n",
      "Train Epoch: 2 [76800/394813 (19%)]\tLoss: 15.220990\n",
      "Train Epoch: 2 [89600/394813 (23%)]\tLoss: 28.671385\n",
      "Train Epoch: 2 [102400/394813 (26%)]\tLoss: 21.133984\n",
      "Train Epoch: 2 [115200/394813 (29%)]\tLoss: 14.105773\n",
      "Train Epoch: 2 [128000/394813 (32%)]\tLoss: 12.989784\n",
      "Train Epoch: 2 [140800/394813 (36%)]\tLoss: 22.048958\n",
      "Train Epoch: 2 [153600/394813 (39%)]\tLoss: 11.527918\n",
      "Train Epoch: 2 [166400/394813 (42%)]\tLoss: 24.568085\n",
      "Train Epoch: 2 [179200/394813 (45%)]\tLoss: 19.137157\n",
      "Train Epoch: 2 [192000/394813 (49%)]\tLoss: 30.630054\n",
      "Train Epoch: 2 [204800/394813 (52%)]\tLoss: 14.675673\n",
      "Train Epoch: 2 [217600/394813 (55%)]\tLoss: 30.695156\n",
      "Train Epoch: 2 [230400/394813 (58%)]\tLoss: 17.811897\n",
      "Train Epoch: 2 [243200/394813 (62%)]\tLoss: 39.810650\n",
      "Train Epoch: 2 [256000/394813 (65%)]\tLoss: 19.866461\n",
      "Train Epoch: 2 [268800/394813 (68%)]\tLoss: 21.427317\n",
      "Train Epoch: 2 [281600/394813 (71%)]\tLoss: 15.959868\n",
      "Train Epoch: 2 [294400/394813 (75%)]\tLoss: 32.650547\n",
      "Train Epoch: 2 [307200/394813 (78%)]\tLoss: 7.757574\n",
      "Train Epoch: 2 [320000/394813 (81%)]\tLoss: 29.640171\n",
      "Train Epoch: 2 [332800/394813 (84%)]\tLoss: 14.837009\n",
      "Train Epoch: 2 [345600/394813 (88%)]\tLoss: 21.227106\n",
      "Train Epoch: 2 [358400/394813 (91%)]\tLoss: 25.321966\n",
      "Train Epoch: 2 [371200/394813 (94%)]\tLoss: 21.861317\n",
      "Train Epoch: 2 [384000/394813 (97%)]\tLoss: 3.478889\n",
      "====> Epoch: 2 Average loss: 22.6377\n",
      "Train Epoch: 3 [0/394813 (0%)]\tLoss: 24.848837\n",
      "Train Epoch: 3 [12800/394813 (3%)]\tLoss: 35.005325\n",
      "Train Epoch: 3 [25600/394813 (6%)]\tLoss: 8.270698\n",
      "Train Epoch: 3 [38400/394813 (10%)]\tLoss: 18.711920\n",
      "Train Epoch: 3 [51200/394813 (13%)]\tLoss: 24.109558\n",
      "Train Epoch: 3 [64000/394813 (16%)]\tLoss: 20.597775\n",
      "Train Epoch: 3 [76800/394813 (19%)]\tLoss: 16.238960\n",
      "Train Epoch: 3 [89600/394813 (23%)]\tLoss: 28.209526\n",
      "Train Epoch: 3 [102400/394813 (26%)]\tLoss: 21.811905\n",
      "Train Epoch: 3 [115200/394813 (29%)]\tLoss: 15.279588\n",
      "Train Epoch: 3 [128000/394813 (32%)]\tLoss: 14.721376\n",
      "Train Epoch: 3 [140800/394813 (36%)]\tLoss: 22.175257\n",
      "Train Epoch: 3 [153600/394813 (39%)]\tLoss: 12.299179\n",
      "Train Epoch: 3 [166400/394813 (42%)]\tLoss: 24.494808\n",
      "Train Epoch: 3 [179200/394813 (45%)]\tLoss: 20.329803\n",
      "Train Epoch: 3 [192000/394813 (49%)]\tLoss: 30.281380\n",
      "Train Epoch: 3 [204800/394813 (52%)]\tLoss: 16.173836\n",
      "Train Epoch: 3 [217600/394813 (55%)]\tLoss: 31.692535\n",
      "Train Epoch: 3 [230400/394813 (58%)]\tLoss: 18.956211\n",
      "Train Epoch: 3 [243200/394813 (62%)]\tLoss: 40.417862\n",
      "Train Epoch: 3 [256000/394813 (65%)]\tLoss: 21.033993\n",
      "Train Epoch: 3 [268800/394813 (68%)]\tLoss: 22.955776\n",
      "Train Epoch: 3 [281600/394813 (71%)]\tLoss: 17.666359\n",
      "Train Epoch: 3 [294400/394813 (75%)]\tLoss: 33.144608\n",
      "Train Epoch: 3 [307200/394813 (78%)]\tLoss: 9.167910\n",
      "Train Epoch: 3 [320000/394813 (81%)]\tLoss: 26.600929\n",
      "Train Epoch: 3 [332800/394813 (84%)]\tLoss: 16.648811\n",
      "Train Epoch: 3 [345600/394813 (88%)]\tLoss: 23.225712\n",
      "Train Epoch: 3 [358400/394813 (91%)]\tLoss: 27.760029\n",
      "Train Epoch: 3 [371200/394813 (94%)]\tLoss: 24.271168\n",
      "Train Epoch: 3 [384000/394813 (97%)]\tLoss: 4.476823\n",
      "====> Epoch: 3 Average loss: 23.1831\n",
      "Train Epoch: 4 [0/394813 (0%)]\tLoss: 25.747097\n",
      "Train Epoch: 4 [12800/394813 (3%)]\tLoss: 35.907310\n",
      "Train Epoch: 4 [25600/394813 (6%)]\tLoss: 9.811192\n",
      "Train Epoch: 4 [38400/394813 (10%)]\tLoss: 20.351864\n",
      "Train Epoch: 4 [51200/394813 (13%)]\tLoss: 25.357855\n",
      "Train Epoch: 4 [64000/394813 (16%)]\tLoss: 22.437201\n",
      "Train Epoch: 4 [76800/394813 (19%)]\tLoss: 18.088131\n",
      "Train Epoch: 4 [89600/394813 (23%)]\tLoss: 31.536060\n",
      "Train Epoch: 4 [102400/394813 (26%)]\tLoss: 24.500429\n",
      "Train Epoch: 4 [115200/394813 (29%)]\tLoss: 16.743782\n",
      "Train Epoch: 4 [128000/394813 (32%)]\tLoss: 16.679436\n",
      "Train Epoch: 4 [140800/394813 (36%)]\tLoss: 24.689194\n",
      "Train Epoch: 4 [153600/394813 (39%)]\tLoss: 14.227010\n",
      "Train Epoch: 4 [166400/394813 (42%)]\tLoss: 26.893272\n",
      "Train Epoch: 4 [179200/394813 (45%)]\tLoss: 22.540550\n",
      "Train Epoch: 4 [192000/394813 (49%)]\tLoss: 31.401953\n",
      "Train Epoch: 4 [204800/394813 (52%)]\tLoss: 18.008818\n",
      "Train Epoch: 4 [217600/394813 (55%)]\tLoss: 34.008858\n",
      "Train Epoch: 4 [230400/394813 (58%)]\tLoss: 21.947237\n",
      "Train Epoch: 4 [243200/394813 (62%)]\tLoss: 41.979965\n",
      "Train Epoch: 4 [256000/394813 (65%)]\tLoss: 24.955191\n",
      "Train Epoch: 4 [268800/394813 (68%)]\tLoss: 26.028742\n",
      "Train Epoch: 4 [281600/394813 (71%)]\tLoss: 20.504564\n",
      "Train Epoch: 4 [294400/394813 (75%)]\tLoss: 34.602642\n",
      "Train Epoch: 4 [307200/394813 (78%)]\tLoss: 11.027883\n",
      "Train Epoch: 4 [320000/394813 (81%)]\tLoss: 28.158983\n",
      "Train Epoch: 4 [332800/394813 (84%)]\tLoss: 18.842047\n",
      "Train Epoch: 4 [345600/394813 (88%)]\tLoss: 25.604744\n",
      "Train Epoch: 4 [358400/394813 (91%)]\tLoss: 29.862747\n",
      "Train Epoch: 4 [371200/394813 (94%)]\tLoss: 27.234901\n",
      "Train Epoch: 4 [384000/394813 (97%)]\tLoss: 5.384309\n",
      "====> Epoch: 4 Average loss: 25.3227\n",
      "Train Epoch: 5 [0/394813 (0%)]\tLoss: 27.843880\n",
      "Train Epoch: 5 [12800/394813 (3%)]\tLoss: 39.014812\n",
      "Train Epoch: 5 [25600/394813 (6%)]\tLoss: 10.672880\n",
      "Train Epoch: 5 [38400/394813 (10%)]\tLoss: 22.937262\n",
      "Train Epoch: 5 [51200/394813 (13%)]\tLoss: 28.465946\n",
      "Train Epoch: 5 [64000/394813 (16%)]\tLoss: 24.988194\n",
      "Train Epoch: 5 [76800/394813 (19%)]\tLoss: 20.446014\n",
      "Train Epoch: 5 [89600/394813 (23%)]\tLoss: 33.139595\n",
      "Train Epoch: 5 [102400/394813 (26%)]\tLoss: 25.865175\n",
      "Train Epoch: 5 [115200/394813 (29%)]\tLoss: 19.075878\n",
      "Train Epoch: 5 [128000/394813 (32%)]\tLoss: 18.990948\n",
      "Train Epoch: 5 [140800/394813 (36%)]\tLoss: 26.926132\n",
      "Train Epoch: 5 [153600/394813 (39%)]\tLoss: 16.073887\n",
      "Train Epoch: 5 [166400/394813 (42%)]\tLoss: 29.759445\n",
      "Train Epoch: 5 [179200/394813 (45%)]\tLoss: 24.953758\n",
      "Train Epoch: 5 [192000/394813 (49%)]\tLoss: 34.363533\n",
      "Train Epoch: 5 [204800/394813 (52%)]\tLoss: 19.880186\n",
      "Train Epoch: 5 [217600/394813 (55%)]\tLoss: 37.677486\n",
      "Train Epoch: 5 [230400/394813 (58%)]\tLoss: 24.152285\n",
      "Train Epoch: 5 [243200/394813 (62%)]\tLoss: 45.215599\n",
      "Train Epoch: 5 [256000/394813 (65%)]\tLoss: 24.715626\n",
      "Train Epoch: 5 [268800/394813 (68%)]\tLoss: 27.967703\n",
      "Train Epoch: 5 [281600/394813 (71%)]\tLoss: 22.584991\n",
      "Train Epoch: 5 [294400/394813 (75%)]\tLoss: 37.852165\n",
      "Train Epoch: 5 [307200/394813 (78%)]\tLoss: 12.800594\n",
      "Train Epoch: 5 [320000/394813 (81%)]\tLoss: 31.080570\n",
      "Train Epoch: 5 [332800/394813 (84%)]\tLoss: 21.117535\n",
      "Train Epoch: 5 [345600/394813 (88%)]\tLoss: 28.945652\n",
      "Train Epoch: 5 [358400/394813 (91%)]\tLoss: 32.204144\n",
      "Train Epoch: 5 [371200/394813 (94%)]\tLoss: 29.813656\n",
      "Train Epoch: 5 [384000/394813 (97%)]\tLoss: 6.135292\n",
      "====> Epoch: 5 Average loss: 27.7006\n",
      "Train Epoch: 6 [0/394813 (0%)]\tLoss: 30.207855\n",
      "Train Epoch: 6 [12800/394813 (3%)]\tLoss: 40.879971\n",
      "Train Epoch: 6 [25600/394813 (6%)]\tLoss: 12.137699\n",
      "Train Epoch: 6 [38400/394813 (10%)]\tLoss: 24.888187\n",
      "Train Epoch: 6 [51200/394813 (13%)]\tLoss: 30.643780\n",
      "Train Epoch: 6 [64000/394813 (16%)]\tLoss: 26.666695\n",
      "Train Epoch: 6 [76800/394813 (19%)]\tLoss: 22.135529\n",
      "Train Epoch: 6 [89600/394813 (23%)]\tLoss: 35.764210\n",
      "Train Epoch: 6 [102400/394813 (26%)]\tLoss: 29.576832\n",
      "Train Epoch: 6 [115200/394813 (29%)]\tLoss: 20.954594\n",
      "Train Epoch: 6 [128000/394813 (32%)]\tLoss: 20.476339\n",
      "Train Epoch: 6 [140800/394813 (36%)]\tLoss: 29.466385\n",
      "Train Epoch: 6 [153600/394813 (39%)]\tLoss: 17.367390\n",
      "Train Epoch: 6 [166400/394813 (42%)]\tLoss: 31.762413\n",
      "Train Epoch: 6 [179200/394813 (45%)]\tLoss: 27.335773\n",
      "Train Epoch: 6 [192000/394813 (49%)]\tLoss: 37.423786\n",
      "Train Epoch: 6 [204800/394813 (52%)]\tLoss: 21.808048\n",
      "Train Epoch: 6 [217600/394813 (55%)]\tLoss: 39.091393\n",
      "Train Epoch: 6 [230400/394813 (58%)]\tLoss: 26.490618\n",
      "Train Epoch: 6 [243200/394813 (62%)]\tLoss: 47.913765\n",
      "Train Epoch: 6 [256000/394813 (65%)]\tLoss: 27.718239\n",
      "Train Epoch: 6 [268800/394813 (68%)]\tLoss: 30.771210\n",
      "Train Epoch: 6 [281600/394813 (71%)]\tLoss: 24.757088\n",
      "Train Epoch: 6 [294400/394813 (75%)]\tLoss: 41.592678\n",
      "Train Epoch: 6 [307200/394813 (78%)]\tLoss: 14.312967\n",
      "Train Epoch: 6 [320000/394813 (81%)]\tLoss: 33.923180\n",
      "Train Epoch: 6 [332800/394813 (84%)]\tLoss: 23.791210\n",
      "Train Epoch: 6 [345600/394813 (88%)]\tLoss: 32.505859\n",
      "Train Epoch: 6 [358400/394813 (91%)]\tLoss: 35.374031\n",
      "Train Epoch: 6 [371200/394813 (94%)]\tLoss: 32.784100\n",
      "Train Epoch: 6 [384000/394813 (97%)]\tLoss: 6.372768\n",
      "====> Epoch: 6 Average loss: 30.0134\n",
      "Train Epoch: 7 [0/394813 (0%)]\tLoss: 32.803936\n",
      "Train Epoch: 7 [12800/394813 (3%)]\tLoss: 43.123817\n",
      "Train Epoch: 7 [25600/394813 (6%)]\tLoss: 12.904216\n",
      "Train Epoch: 7 [38400/394813 (10%)]\tLoss: 26.455719\n",
      "Train Epoch: 7 [51200/394813 (13%)]\tLoss: 32.428223\n",
      "Train Epoch: 7 [64000/394813 (16%)]\tLoss: 29.117594\n",
      "Train Epoch: 7 [76800/394813 (19%)]\tLoss: 24.492153\n",
      "Train Epoch: 7 [89600/394813 (23%)]\tLoss: 38.351212\n",
      "Train Epoch: 7 [102400/394813 (26%)]\tLoss: 31.358772\n",
      "Train Epoch: 7 [115200/394813 (29%)]\tLoss: 22.344057\n",
      "Train Epoch: 7 [128000/394813 (32%)]\tLoss: 23.130394\n",
      "Train Epoch: 7 [140800/394813 (36%)]\tLoss: 32.357666\n",
      "Train Epoch: 7 [153600/394813 (39%)]\tLoss: 18.340008\n",
      "Train Epoch: 7 [166400/394813 (42%)]\tLoss: 34.402111\n",
      "Train Epoch: 7 [179200/394813 (45%)]\tLoss: 31.037310\n",
      "Train Epoch: 7 [192000/394813 (49%)]\tLoss: 39.589211\n",
      "Train Epoch: 7 [204800/394813 (52%)]\tLoss: 23.273575\n",
      "Train Epoch: 7 [217600/394813 (55%)]\tLoss: 41.586266\n",
      "Train Epoch: 7 [230400/394813 (58%)]\tLoss: 28.514921\n",
      "Train Epoch: 7 [243200/394813 (62%)]\tLoss: 50.130234\n",
      "Train Epoch: 7 [256000/394813 (65%)]\tLoss: 34.441788\n",
      "Train Epoch: 7 [268800/394813 (68%)]\tLoss: 33.524864\n",
      "Train Epoch: 7 [281600/394813 (71%)]\tLoss: 25.452898\n",
      "Train Epoch: 7 [294400/394813 (75%)]\tLoss: 43.648239\n",
      "Train Epoch: 7 [307200/394813 (78%)]\tLoss: 15.299219\n",
      "Train Epoch: 7 [320000/394813 (81%)]\tLoss: 36.425835\n",
      "Train Epoch: 7 [332800/394813 (84%)]\tLoss: 25.492657\n",
      "Train Epoch: 7 [345600/394813 (88%)]\tLoss: 34.731682\n",
      "Train Epoch: 7 [358400/394813 (91%)]\tLoss: 37.906082\n",
      "Train Epoch: 7 [371200/394813 (94%)]\tLoss: 34.591606\n",
      "Train Epoch: 7 [384000/394813 (97%)]\tLoss: 6.961009\n",
      "====> Epoch: 7 Average loss: 32.2241\n",
      "Train Epoch: 8 [0/394813 (0%)]\tLoss: 35.583755\n",
      "Train Epoch: 8 [12800/394813 (3%)]\tLoss: 44.320969\n",
      "Train Epoch: 8 [25600/394813 (6%)]\tLoss: 14.172943\n",
      "Train Epoch: 8 [38400/394813 (10%)]\tLoss: 28.483341\n",
      "Train Epoch: 8 [51200/394813 (13%)]\tLoss: 34.941463\n",
      "Train Epoch: 8 [64000/394813 (16%)]\tLoss: 30.058424\n",
      "Train Epoch: 8 [76800/394813 (19%)]\tLoss: 26.740427\n",
      "Train Epoch: 8 [89600/394813 (23%)]\tLoss: 41.691040\n",
      "Train Epoch: 8 [102400/394813 (26%)]\tLoss: 33.399052\n",
      "Train Epoch: 8 [115200/394813 (29%)]\tLoss: 24.062981\n",
      "Train Epoch: 8 [128000/394813 (32%)]\tLoss: 24.542511\n",
      "Train Epoch: 8 [140800/394813 (36%)]\tLoss: 34.714962\n",
      "Train Epoch: 8 [153600/394813 (39%)]\tLoss: 20.204693\n",
      "Train Epoch: 8 [166400/394813 (42%)]\tLoss: 36.970299\n",
      "Train Epoch: 8 [179200/394813 (45%)]\tLoss: 31.901836\n",
      "Train Epoch: 8 [192000/394813 (49%)]\tLoss: 41.832329\n",
      "Train Epoch: 8 [204800/394813 (52%)]\tLoss: 24.570679\n",
      "Train Epoch: 8 [217600/394813 (55%)]\tLoss: 44.110359\n",
      "Train Epoch: 8 [230400/394813 (58%)]\tLoss: 30.286556\n",
      "Train Epoch: 8 [243200/394813 (62%)]\tLoss: 53.565853\n",
      "Train Epoch: 8 [256000/394813 (65%)]\tLoss: 31.361151\n",
      "Train Epoch: 8 [268800/394813 (68%)]\tLoss: 36.952335\n",
      "Train Epoch: 8 [281600/394813 (71%)]\tLoss: 27.936089\n",
      "Train Epoch: 8 [294400/394813 (75%)]\tLoss: 46.779984\n",
      "Train Epoch: 8 [307200/394813 (78%)]\tLoss: 16.724699\n",
      "Train Epoch: 8 [320000/394813 (81%)]\tLoss: 39.388206\n",
      "Train Epoch: 8 [332800/394813 (84%)]\tLoss: 28.538769\n",
      "Train Epoch: 8 [345600/394813 (88%)]\tLoss: 36.443104\n",
      "Train Epoch: 8 [358400/394813 (91%)]\tLoss: 40.689838\n",
      "Train Epoch: 8 [371200/394813 (94%)]\tLoss: 35.905891\n",
      "Train Epoch: 8 [384000/394813 (97%)]\tLoss: 6.760062\n",
      "====> Epoch: 8 Average loss: 34.2564\n",
      "Train Epoch: 9 [0/394813 (0%)]\tLoss: 37.274502\n",
      "Train Epoch: 9 [12800/394813 (3%)]\tLoss: 46.058212\n",
      "Train Epoch: 9 [25600/394813 (6%)]\tLoss: 14.217424\n",
      "Train Epoch: 9 [38400/394813 (10%)]\tLoss: 30.810318\n",
      "Train Epoch: 9 [51200/394813 (13%)]\tLoss: 36.823532\n",
      "Train Epoch: 9 [64000/394813 (16%)]\tLoss: 32.502190\n",
      "Train Epoch: 9 [76800/394813 (19%)]\tLoss: 28.361849\n",
      "Train Epoch: 9 [89600/394813 (23%)]\tLoss: 43.293854\n",
      "Train Epoch: 9 [102400/394813 (26%)]\tLoss: 35.998184\n",
      "Train Epoch: 9 [115200/394813 (29%)]\tLoss: 25.741934\n",
      "Train Epoch: 9 [128000/394813 (32%)]\tLoss: 25.771368\n",
      "Train Epoch: 9 [140800/394813 (36%)]\tLoss: 37.466507\n",
      "Train Epoch: 9 [153600/394813 (39%)]\tLoss: 21.733837\n",
      "Train Epoch: 9 [166400/394813 (42%)]\tLoss: 38.764050\n",
      "Train Epoch: 9 [179200/394813 (45%)]\tLoss: 35.251862\n",
      "Train Epoch: 9 [192000/394813 (49%)]\tLoss: 44.691963\n",
      "Train Epoch: 9 [204800/394813 (52%)]\tLoss: 26.170952\n",
      "Train Epoch: 9 [217600/394813 (55%)]\tLoss: 46.969063\n",
      "Train Epoch: 9 [230400/394813 (58%)]\tLoss: 31.827932\n",
      "Train Epoch: 9 [243200/394813 (62%)]\tLoss: 54.901802\n",
      "Train Epoch: 9 [256000/394813 (65%)]\tLoss: 33.551910\n",
      "Train Epoch: 9 [268800/394813 (68%)]\tLoss: 38.336067\n",
      "Train Epoch: 9 [281600/394813 (71%)]\tLoss: 29.050390\n",
      "Train Epoch: 9 [294400/394813 (75%)]\tLoss: 48.891621\n",
      "Train Epoch: 9 [307200/394813 (78%)]\tLoss: 17.435974\n",
      "Train Epoch: 9 [320000/394813 (81%)]\tLoss: 41.335178\n",
      "Train Epoch: 9 [332800/394813 (84%)]\tLoss: 29.582617\n",
      "Train Epoch: 9 [345600/394813 (88%)]\tLoss: 39.858631\n",
      "Train Epoch: 9 [358400/394813 (91%)]\tLoss: 42.096897\n",
      "Train Epoch: 9 [371200/394813 (94%)]\tLoss: 39.193153\n",
      "Train Epoch: 9 [384000/394813 (97%)]\tLoss: 7.267363\n",
      "====> Epoch: 9 Average loss: 36.1936\n",
      "Train Epoch: 10 [0/394813 (0%)]\tLoss: 39.489594\n",
      "Train Epoch: 10 [12800/394813 (3%)]\tLoss: 48.752949\n",
      "Train Epoch: 10 [25600/394813 (6%)]\tLoss: 15.624720\n",
      "Train Epoch: 10 [38400/394813 (10%)]\tLoss: 32.433662\n",
      "Train Epoch: 10 [51200/394813 (13%)]\tLoss: 38.312065\n",
      "Train Epoch: 10 [64000/394813 (16%)]\tLoss: 34.284916\n",
      "Train Epoch: 10 [76800/394813 (19%)]\tLoss: 29.335758\n",
      "Train Epoch: 10 [89600/394813 (23%)]\tLoss: 45.272400\n",
      "Train Epoch: 10 [102400/394813 (26%)]\tLoss: 38.290199\n",
      "Train Epoch: 10 [115200/394813 (29%)]\tLoss: 26.693270\n",
      "Train Epoch: 10 [128000/394813 (32%)]\tLoss: 27.773300\n",
      "Train Epoch: 10 [140800/394813 (36%)]\tLoss: 38.961281\n",
      "Train Epoch: 10 [153600/394813 (39%)]\tLoss: 22.719978\n",
      "Train Epoch: 10 [166400/394813 (42%)]\tLoss: 41.018661\n",
      "Train Epoch: 10 [179200/394813 (45%)]\tLoss: 36.767006\n",
      "Train Epoch: 10 [192000/394813 (49%)]\tLoss: 45.958015\n",
      "Train Epoch: 10 [204800/394813 (52%)]\tLoss: 27.461250\n",
      "Train Epoch: 10 [217600/394813 (55%)]\tLoss: 48.264431\n",
      "Train Epoch: 10 [230400/394813 (58%)]\tLoss: 33.746063\n",
      "Train Epoch: 10 [243200/394813 (62%)]\tLoss: 57.620750\n",
      "Train Epoch: 10 [256000/394813 (65%)]\tLoss: 33.959991\n",
      "Train Epoch: 10 [268800/394813 (68%)]\tLoss: 40.520164\n",
      "Train Epoch: 10 [281600/394813 (71%)]\tLoss: 31.416050\n",
      "Train Epoch: 10 [294400/394813 (75%)]\tLoss: 52.546654\n",
      "Train Epoch: 10 [307200/394813 (78%)]\tLoss: 17.771307\n",
      "Train Epoch: 10 [320000/394813 (81%)]\tLoss: 43.046738\n",
      "Train Epoch: 10 [332800/394813 (84%)]\tLoss: 31.568230\n",
      "Train Epoch: 10 [345600/394813 (88%)]\tLoss: 42.037704\n",
      "Train Epoch: 10 [358400/394813 (91%)]\tLoss: 45.161289\n",
      "Train Epoch: 10 [371200/394813 (94%)]\tLoss: 40.330025\n",
      "Train Epoch: 10 [384000/394813 (97%)]\tLoss: 7.307196\n",
      "====> Epoch: 10 Average loss: 37.9697\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./model_realbook.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vérité\n",
      "['C:maj', 'C:maj', 'C:maj', 'C:maj', 'C:7', 'C:7', 'C:7', 'C:7', 'F:maj', 'F:maj', 'F:maj', 'F:maj', 'F:min7', 'F:min7', 'F:min7', 'F:min7']\n",
      "\n",
      "Par VAE\n",
      "['G:7', 'C:maj', 'C:maj', 'C:maj', 'C:7', 'C:7', 'C:7', 'C:7', 'F:maj', 'F:maj', 'F:maj', 'F:maj', 'F:maj', 'F:maj', 'F:min7', 'F:min7']\n"
     ]
    }
   ],
   "source": [
    "PITCH_LIST = [\"A\", \"A#\", \"B\", \"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\"]\n",
    "QUALITY_LIST = [\"maj\", \"min\", \"dim\", \"maj7\", \"min7\", \"7\", \"dim7\"]\n",
    "\n",
    "def sample_to_chords(sample):\n",
    "    idx_chords = np.argmax(sample[0,:,:],1)\n",
    "#     print(idx_chords)\n",
    "    chords = [PITCH_LIST[int(idx/7)] + \":\" + QUALITY_LIST[int(idx%7)] for idx in idx_chords]\n",
    "    return chords\n",
    "\n",
    "index_test = 16\n",
    "test_sample = realbook_dataset[0][index_test]\n",
    "\n",
    "print(\"Vérité\")\n",
    "true_sample = test_sample.view(1, 16, -1).numpy()\n",
    "print(sample_to_chords(true_sample))\n",
    "\n",
    "print()\n",
    "print(\"Par VAE\")\n",
    "model.to(torch.device(\"cpu\"))\n",
    "recons_test, _, _ = model(test_sample)\n",
    "print(sample_to_chords(recons_test.detach().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 16, 84)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['C:maj',\n",
       " 'C#:7',\n",
       " 'C#:7',\n",
       " 'C:maj',\n",
       " 'C:maj',\n",
       " 'C:maj',\n",
       " 'C:maj',\n",
       " 'C:maj',\n",
       " 'C:maj',\n",
       " 'C:maj',\n",
       " 'C:min',\n",
       " 'C:maj',\n",
       " 'C:min',\n",
       " 'C#:7',\n",
       " 'C#:7',\n",
       " 'F:min']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_LATENT = 40\n",
    "with torch.no_grad():\n",
    "    sample = torch.randn(1, N_LATENT)\n",
    "    sample = model.decode(sample).cpu()\n",
    "    sample = sample.numpy()\n",
    "print(sample.shape)\n",
    "sample_to_chords(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3085"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
