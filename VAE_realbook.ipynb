{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import data_loader\n",
    "import numpy as np\n",
    "import sample_to_chords as s2c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda:0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():  \n",
    "  dev = \"cuda:0\" \n",
    "else:  \n",
    "  dev = \"cpu\"  \n",
    "print('using',dev)\n",
    "device = torch.device(dev)\n",
    "class VAE(nn.Module):\n",
    "    N_CHORDS = 16\n",
    "    N_PITCH = 12\n",
    "    N_QUALITY = 7 # A changer aussi dans data_loader\n",
    "    \n",
    "    SIZE_HIDDEN = 400\n",
    "    SIZE_LATENT = 40\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(self.N_CHORDS * self.N_PITCH * self.N_QUALITY, self.SIZE_HIDDEN)\n",
    "        self.fc21 = nn.Linear(self.SIZE_HIDDEN, self.SIZE_LATENT)\n",
    "        self.fc22 = nn.Linear(self.SIZE_HIDDEN, self.SIZE_LATENT)\n",
    "        \n",
    "        self.fc3 = nn.Linear(self.SIZE_LATENT, self.SIZE_HIDDEN)\n",
    "        self.fc4 = nn.Linear(self.SIZE_HIDDEN, self.N_CHORDS * self.N_PITCH * self.N_QUALITY)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        soft = nn.Sigmoid()\n",
    "        return soft(self.fc4(h3).view(-1, self.N_CHORDS, self.N_PITCH * self.N_QUALITY))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, self.N_CHORDS*self.N_PITCH * self.N_QUALITY))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar, beta):\n",
    "    BCE = F.binary_cross_entropy(recon_x.view(-1, 16*12*7), x.view(-1, 16*12*7), reduction='sum')\n",
    "\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + beta*KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    beta = epoch/epochs\n",
    "    for batch_idx, data in enumerate(realbook_dataset):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    \n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar, beta)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), Nchunks,\n",
    "                100. * batch_idx * len(data)/ Nchunks,\n",
    "                loss.item() / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / Nchunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 128\n",
    "log_interval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded !\n"
     ]
    }
   ],
   "source": [
    "realbook_dataset = data_loader.import_dataset()\n",
    "Nchunks = len(realbook_dataset)\n",
    "realbook_dataset = torch.split(realbook_dataset, batch_size, 0)\n",
    "model = VAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"./model_realbook.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/394813 (0%)]\tLoss: 943.266479\n",
      "Train Epoch: 1 [12800/394813 (3%)]\tLoss: 93.842445\n",
      "Train Epoch: 1 [25600/394813 (6%)]\tLoss: 34.042294\n",
      "Train Epoch: 1 [38400/394813 (10%)]\tLoss: 61.024815\n",
      "Train Epoch: 1 [51200/394813 (13%)]\tLoss: 60.607151\n",
      "Train Epoch: 1 [64000/394813 (16%)]\tLoss: 54.269890\n",
      "Train Epoch: 1 [76800/394813 (19%)]\tLoss: 41.321392\n",
      "Train Epoch: 1 [89600/394813 (23%)]\tLoss: 56.754173\n",
      "Train Epoch: 1 [102400/394813 (26%)]\tLoss: 45.376827\n",
      "Train Epoch: 1 [115200/394813 (29%)]\tLoss: 26.551548\n",
      "Train Epoch: 1 [128000/394813 (32%)]\tLoss: 24.273552\n",
      "Train Epoch: 1 [140800/394813 (36%)]\tLoss: 40.978752\n",
      "Train Epoch: 1 [153600/394813 (39%)]\tLoss: 20.575209\n",
      "Train Epoch: 1 [166400/394813 (42%)]\tLoss: 36.081661\n",
      "Train Epoch: 1 [179200/394813 (45%)]\tLoss: 26.681105\n",
      "Train Epoch: 1 [192000/394813 (49%)]\tLoss: 47.642391\n",
      "Train Epoch: 1 [204800/394813 (52%)]\tLoss: 17.092718\n",
      "Train Epoch: 1 [217600/394813 (55%)]\tLoss: 39.853134\n",
      "Train Epoch: 1 [230400/394813 (58%)]\tLoss: 22.968006\n",
      "Train Epoch: 1 [243200/394813 (62%)]\tLoss: 51.721130\n",
      "Train Epoch: 1 [256000/394813 (65%)]\tLoss: 23.448956\n",
      "Train Epoch: 1 [268800/394813 (68%)]\tLoss: 27.713676\n",
      "Train Epoch: 1 [281600/394813 (71%)]\tLoss: 18.537539\n",
      "Train Epoch: 1 [294400/394813 (75%)]\tLoss: 36.347458\n",
      "Train Epoch: 1 [307200/394813 (78%)]\tLoss: 6.849774\n",
      "Train Epoch: 1 [320000/394813 (81%)]\tLoss: 49.697865\n",
      "Train Epoch: 1 [332800/394813 (84%)]\tLoss: 15.791502\n",
      "Train Epoch: 1 [345600/394813 (88%)]\tLoss: 25.073586\n",
      "Train Epoch: 1 [358400/394813 (91%)]\tLoss: 30.482319\n",
      "Train Epoch: 1 [371200/394813 (94%)]\tLoss: 23.831799\n",
      "Train Epoch: 1 [384000/394813 (97%)]\tLoss: 2.786595\n",
      "====> Epoch: 1 Average loss: 40.3773\n",
      "Train Epoch: 2 [0/394813 (0%)]\tLoss: 27.689392\n",
      "Train Epoch: 2 [12800/394813 (3%)]\tLoss: 37.892879\n",
      "Train Epoch: 2 [25600/394813 (6%)]\tLoss: 7.492609\n",
      "Train Epoch: 2 [38400/394813 (10%)]\tLoss: 18.061813\n",
      "Train Epoch: 2 [51200/394813 (13%)]\tLoss: 23.538815\n",
      "Train Epoch: 2 [64000/394813 (16%)]\tLoss: 20.448349\n",
      "Train Epoch: 2 [76800/394813 (19%)]\tLoss: 15.002057\n",
      "Train Epoch: 2 [89600/394813 (23%)]\tLoss: 28.632221\n",
      "Train Epoch: 2 [102400/394813 (26%)]\tLoss: 20.471319\n",
      "Train Epoch: 2 [115200/394813 (29%)]\tLoss: 14.114943\n",
      "Train Epoch: 2 [128000/394813 (32%)]\tLoss: 13.237054\n",
      "Train Epoch: 2 [140800/394813 (36%)]\tLoss: 21.064919\n",
      "Train Epoch: 2 [153600/394813 (39%)]\tLoss: 11.163370\n",
      "Train Epoch: 2 [166400/394813 (42%)]\tLoss: 24.478073\n",
      "Train Epoch: 2 [179200/394813 (45%)]\tLoss: 17.947540\n",
      "Train Epoch: 2 [192000/394813 (49%)]\tLoss: 30.999804\n",
      "Train Epoch: 2 [204800/394813 (52%)]\tLoss: 14.399643\n",
      "Train Epoch: 2 [217600/394813 (55%)]\tLoss: 31.003920\n",
      "Train Epoch: 2 [230400/394813 (58%)]\tLoss: 19.451809\n",
      "Train Epoch: 2 [243200/394813 (62%)]\tLoss: 43.093395\n",
      "Train Epoch: 2 [256000/394813 (65%)]\tLoss: 18.562016\n",
      "Train Epoch: 2 [268800/394813 (68%)]\tLoss: 21.880924\n",
      "Train Epoch: 2 [281600/394813 (71%)]\tLoss: 15.738960\n",
      "Train Epoch: 2 [294400/394813 (75%)]\tLoss: 30.079144\n",
      "Train Epoch: 2 [307200/394813 (78%)]\tLoss: 7.672157\n",
      "Train Epoch: 2 [320000/394813 (81%)]\tLoss: 29.431456\n",
      "Train Epoch: 2 [332800/394813 (84%)]\tLoss: 15.243237\n",
      "Train Epoch: 2 [345600/394813 (88%)]\tLoss: 21.115709\n",
      "Train Epoch: 2 [358400/394813 (91%)]\tLoss: 26.489838\n",
      "Train Epoch: 2 [371200/394813 (94%)]\tLoss: 21.190353\n",
      "Train Epoch: 2 [384000/394813 (97%)]\tLoss: 3.999488\n",
      "====> Epoch: 2 Average loss: 22.4668\n",
      "Train Epoch: 3 [0/394813 (0%)]\tLoss: 24.180634\n",
      "Train Epoch: 3 [12800/394813 (3%)]\tLoss: 32.769169\n",
      "Train Epoch: 3 [25600/394813 (6%)]\tLoss: 8.215457\n",
      "Train Epoch: 3 [38400/394813 (10%)]\tLoss: 18.883921\n",
      "Train Epoch: 3 [51200/394813 (13%)]\tLoss: 23.188519\n",
      "Train Epoch: 3 [64000/394813 (16%)]\tLoss: 20.458214\n",
      "Train Epoch: 3 [76800/394813 (19%)]\tLoss: 15.928913\n",
      "Train Epoch: 3 [89600/394813 (23%)]\tLoss: 28.718651\n",
      "Train Epoch: 3 [102400/394813 (26%)]\tLoss: 21.433811\n",
      "Train Epoch: 3 [115200/394813 (29%)]\tLoss: 15.502217\n",
      "Train Epoch: 3 [128000/394813 (32%)]\tLoss: 14.596581\n",
      "Train Epoch: 3 [140800/394813 (36%)]\tLoss: 20.908611\n",
      "Train Epoch: 3 [153600/394813 (39%)]\tLoss: 12.548635\n",
      "Train Epoch: 3 [166400/394813 (42%)]\tLoss: 24.358646\n",
      "Train Epoch: 3 [179200/394813 (45%)]\tLoss: 19.516159\n",
      "Train Epoch: 3 [192000/394813 (49%)]\tLoss: 30.548311\n",
      "Train Epoch: 3 [204800/394813 (52%)]\tLoss: 16.187140\n",
      "Train Epoch: 3 [217600/394813 (55%)]\tLoss: 31.588177\n",
      "Train Epoch: 3 [230400/394813 (58%)]\tLoss: 20.164833\n",
      "Train Epoch: 3 [243200/394813 (62%)]\tLoss: 43.378323\n",
      "Train Epoch: 3 [256000/394813 (65%)]\tLoss: 21.017265\n",
      "Train Epoch: 3 [268800/394813 (68%)]\tLoss: 23.452007\n",
      "Train Epoch: 3 [281600/394813 (71%)]\tLoss: 17.620766\n",
      "Train Epoch: 3 [294400/394813 (75%)]\tLoss: 31.497528\n",
      "Train Epoch: 3 [307200/394813 (78%)]\tLoss: 9.127121\n",
      "Train Epoch: 3 [320000/394813 (81%)]\tLoss: 28.045406\n",
      "Train Epoch: 3 [332800/394813 (84%)]\tLoss: 17.683617\n",
      "Train Epoch: 3 [345600/394813 (88%)]\tLoss: 23.940548\n",
      "Train Epoch: 3 [358400/394813 (91%)]\tLoss: 27.335327\n",
      "Train Epoch: 3 [371200/394813 (94%)]\tLoss: 23.586895\n",
      "Train Epoch: 3 [384000/394813 (97%)]\tLoss: 4.297508\n",
      "====> Epoch: 3 Average loss: 23.1513\n",
      "Train Epoch: 4 [0/394813 (0%)]\tLoss: 25.773277\n",
      "Train Epoch: 4 [12800/394813 (3%)]\tLoss: 33.558651\n",
      "Train Epoch: 4 [25600/394813 (6%)]\tLoss: 9.331999\n",
      "Train Epoch: 4 [38400/394813 (10%)]\tLoss: 19.852455\n",
      "Train Epoch: 4 [51200/394813 (13%)]\tLoss: 25.065313\n",
      "Train Epoch: 4 [64000/394813 (16%)]\tLoss: 22.176485\n",
      "Train Epoch: 4 [76800/394813 (19%)]\tLoss: 18.036892\n",
      "Train Epoch: 4 [89600/394813 (23%)]\tLoss: 31.510036\n",
      "Train Epoch: 4 [102400/394813 (26%)]\tLoss: 24.280239\n",
      "Train Epoch: 4 [115200/394813 (29%)]\tLoss: 17.317781\n",
      "Train Epoch: 4 [128000/394813 (32%)]\tLoss: 16.831848\n",
      "Train Epoch: 4 [140800/394813 (36%)]\tLoss: 23.704727\n",
      "Train Epoch: 4 [153600/394813 (39%)]\tLoss: 14.263168\n",
      "Train Epoch: 4 [166400/394813 (42%)]\tLoss: 26.354725\n",
      "Train Epoch: 4 [179200/394813 (45%)]\tLoss: 22.378803\n",
      "Train Epoch: 4 [192000/394813 (49%)]\tLoss: 31.812412\n",
      "Train Epoch: 4 [204800/394813 (52%)]\tLoss: 17.729294\n",
      "Train Epoch: 4 [217600/394813 (55%)]\tLoss: 34.146500\n",
      "Train Epoch: 4 [230400/394813 (58%)]\tLoss: 23.664810\n",
      "Train Epoch: 4 [243200/394813 (62%)]\tLoss: 45.942066\n",
      "Train Epoch: 4 [256000/394813 (65%)]\tLoss: 23.225430\n",
      "Train Epoch: 4 [268800/394813 (68%)]\tLoss: 26.127247\n",
      "Train Epoch: 4 [281600/394813 (71%)]\tLoss: 19.607933\n",
      "Train Epoch: 4 [294400/394813 (75%)]\tLoss: 34.364914\n",
      "Train Epoch: 4 [307200/394813 (78%)]\tLoss: 10.909077\n",
      "Train Epoch: 4 [320000/394813 (81%)]\tLoss: 29.351532\n",
      "Train Epoch: 4 [332800/394813 (84%)]\tLoss: 20.285954\n",
      "Train Epoch: 4 [345600/394813 (88%)]\tLoss: 26.792671\n",
      "Train Epoch: 4 [358400/394813 (91%)]\tLoss: 30.737244\n",
      "Train Epoch: 4 [371200/394813 (94%)]\tLoss: 25.656176\n",
      "Train Epoch: 4 [384000/394813 (97%)]\tLoss: 4.711894\n",
      "====> Epoch: 4 Average loss: 25.2772\n",
      "Train Epoch: 5 [0/394813 (0%)]\tLoss: 28.179281\n",
      "Train Epoch: 5 [12800/394813 (3%)]\tLoss: 37.151009\n",
      "Train Epoch: 5 [25600/394813 (6%)]\tLoss: 10.339054\n",
      "Train Epoch: 5 [38400/394813 (10%)]\tLoss: 22.580099\n",
      "Train Epoch: 5 [51200/394813 (13%)]\tLoss: 27.555408\n",
      "Train Epoch: 5 [64000/394813 (16%)]\tLoss: 25.052429\n",
      "Train Epoch: 5 [76800/394813 (19%)]\tLoss: 20.860765\n",
      "Train Epoch: 5 [89600/394813 (23%)]\tLoss: 33.723236\n",
      "Train Epoch: 5 [102400/394813 (26%)]\tLoss: 26.520844\n",
      "Train Epoch: 5 [115200/394813 (29%)]\tLoss: 19.386614\n",
      "Train Epoch: 5 [128000/394813 (32%)]\tLoss: 18.954302\n",
      "Train Epoch: 5 [140800/394813 (36%)]\tLoss: 26.717316\n",
      "Train Epoch: 5 [153600/394813 (39%)]\tLoss: 15.545140\n",
      "Train Epoch: 5 [166400/394813 (42%)]\tLoss: 28.916821\n",
      "Train Epoch: 5 [179200/394813 (45%)]\tLoss: 25.341621\n",
      "Train Epoch: 5 [192000/394813 (49%)]\tLoss: 34.532127\n",
      "Train Epoch: 5 [204800/394813 (52%)]\tLoss: 19.580923\n",
      "Train Epoch: 5 [217600/394813 (55%)]\tLoss: 37.025841\n",
      "Train Epoch: 5 [230400/394813 (58%)]\tLoss: 25.347519\n",
      "Train Epoch: 5 [243200/394813 (62%)]\tLoss: 47.384468\n",
      "Train Epoch: 5 [256000/394813 (65%)]\tLoss: 27.639786\n",
      "Train Epoch: 5 [268800/394813 (68%)]\tLoss: 29.196714\n",
      "Train Epoch: 5 [281600/394813 (71%)]\tLoss: 21.595459\n",
      "Train Epoch: 5 [294400/394813 (75%)]\tLoss: 37.922211\n",
      "Train Epoch: 5 [307200/394813 (78%)]\tLoss: 12.054067\n",
      "Train Epoch: 5 [320000/394813 (81%)]\tLoss: 32.286705\n",
      "Train Epoch: 5 [332800/394813 (84%)]\tLoss: 22.763493\n",
      "Train Epoch: 5 [345600/394813 (88%)]\tLoss: 29.822731\n",
      "Train Epoch: 5 [358400/394813 (91%)]\tLoss: 33.733421\n",
      "Train Epoch: 5 [371200/394813 (94%)]\tLoss: 28.432274\n",
      "Train Epoch: 5 [384000/394813 (97%)]\tLoss: 5.308343\n",
      "====> Epoch: 5 Average loss: 27.6773\n",
      "Train Epoch: 6 [0/394813 (0%)]\tLoss: 30.592831\n",
      "Train Epoch: 6 [12800/394813 (3%)]\tLoss: 39.830231\n",
      "Train Epoch: 6 [25600/394813 (6%)]\tLoss: 10.937389\n",
      "Train Epoch: 6 [38400/394813 (10%)]\tLoss: 24.824417\n",
      "Train Epoch: 6 [51200/394813 (13%)]\tLoss: 30.115059\n",
      "Train Epoch: 6 [64000/394813 (16%)]\tLoss: 25.904812\n",
      "Train Epoch: 6 [76800/394813 (19%)]\tLoss: 22.423393\n",
      "Train Epoch: 6 [89600/394813 (23%)]\tLoss: 35.612816\n",
      "Train Epoch: 6 [102400/394813 (26%)]\tLoss: 29.446301\n",
      "Train Epoch: 6 [115200/394813 (29%)]\tLoss: 21.363567\n",
      "Train Epoch: 6 [128000/394813 (32%)]\tLoss: 20.709822\n",
      "Train Epoch: 6 [140800/394813 (36%)]\tLoss: 28.441008\n",
      "Train Epoch: 6 [153600/394813 (39%)]\tLoss: 17.942581\n",
      "Train Epoch: 6 [166400/394813 (42%)]\tLoss: 32.148651\n",
      "Train Epoch: 6 [179200/394813 (45%)]\tLoss: 27.150360\n",
      "Train Epoch: 6 [192000/394813 (49%)]\tLoss: 37.575119\n",
      "Train Epoch: 6 [204800/394813 (52%)]\tLoss: 21.487820\n",
      "Train Epoch: 6 [217600/394813 (55%)]\tLoss: 39.250175\n",
      "Train Epoch: 6 [230400/394813 (58%)]\tLoss: 27.891626\n",
      "Train Epoch: 6 [243200/394813 (62%)]\tLoss: 50.071915\n",
      "Train Epoch: 6 [256000/394813 (65%)]\tLoss: 25.864994\n",
      "Train Epoch: 6 [268800/394813 (68%)]\tLoss: 31.917870\n",
      "Train Epoch: 6 [281600/394813 (71%)]\tLoss: 23.391129\n",
      "Train Epoch: 6 [294400/394813 (75%)]\tLoss: 40.547211\n",
      "Train Epoch: 6 [307200/394813 (78%)]\tLoss: 13.449333\n",
      "Train Epoch: 6 [320000/394813 (81%)]\tLoss: 34.587418\n",
      "Train Epoch: 6 [332800/394813 (84%)]\tLoss: 24.607143\n",
      "Train Epoch: 6 [345600/394813 (88%)]\tLoss: 32.777184\n",
      "Train Epoch: 6 [358400/394813 (91%)]\tLoss: 36.234962\n",
      "Train Epoch: 6 [371200/394813 (94%)]\tLoss: 31.527279\n",
      "Train Epoch: 6 [384000/394813 (97%)]\tLoss: 5.965994\n",
      "====> Epoch: 6 Average loss: 29.9735\n",
      "Train Epoch: 7 [0/394813 (0%)]\tLoss: 32.197166\n",
      "Train Epoch: 7 [12800/394813 (3%)]\tLoss: 41.749420\n",
      "Train Epoch: 7 [25600/394813 (6%)]\tLoss: 12.382493\n",
      "Train Epoch: 7 [38400/394813 (10%)]\tLoss: 27.403259\n",
      "Train Epoch: 7 [51200/394813 (13%)]\tLoss: 32.547554\n",
      "Train Epoch: 7 [64000/394813 (16%)]\tLoss: 28.524857\n",
      "Train Epoch: 7 [76800/394813 (19%)]\tLoss: 24.070227\n",
      "Train Epoch: 7 [89600/394813 (23%)]\tLoss: 38.461273\n",
      "Train Epoch: 7 [102400/394813 (26%)]\tLoss: 32.025581\n",
      "Train Epoch: 7 [115200/394813 (29%)]\tLoss: 23.148888\n",
      "Train Epoch: 7 [128000/394813 (32%)]\tLoss: 22.575384\n",
      "Train Epoch: 7 [140800/394813 (36%)]\tLoss: 30.952736\n",
      "Train Epoch: 7 [153600/394813 (39%)]\tLoss: 18.997309\n",
      "Train Epoch: 7 [166400/394813 (42%)]\tLoss: 34.546722\n",
      "Train Epoch: 7 [179200/394813 (45%)]\tLoss: 29.701620\n",
      "Train Epoch: 7 [192000/394813 (49%)]\tLoss: 39.869827\n",
      "Train Epoch: 7 [204800/394813 (52%)]\tLoss: 23.072853\n",
      "Train Epoch: 7 [217600/394813 (55%)]\tLoss: 41.893227\n",
      "Train Epoch: 7 [230400/394813 (58%)]\tLoss: 29.606457\n",
      "Train Epoch: 7 [243200/394813 (62%)]\tLoss: 52.654572\n",
      "Train Epoch: 7 [256000/394813 (65%)]\tLoss: 28.271736\n",
      "Train Epoch: 7 [268800/394813 (68%)]\tLoss: 34.356468\n",
      "Train Epoch: 7 [281600/394813 (71%)]\tLoss: 25.977932\n",
      "Train Epoch: 7 [294400/394813 (75%)]\tLoss: 44.515213\n",
      "Train Epoch: 7 [307200/394813 (78%)]\tLoss: 14.196795\n",
      "Train Epoch: 7 [320000/394813 (81%)]\tLoss: 37.583656\n",
      "Train Epoch: 7 [332800/394813 (84%)]\tLoss: 26.587576\n",
      "Train Epoch: 7 [345600/394813 (88%)]\tLoss: 34.838673\n",
      "Train Epoch: 7 [358400/394813 (91%)]\tLoss: 39.647083\n",
      "Train Epoch: 7 [371200/394813 (94%)]\tLoss: 32.979370\n",
      "Train Epoch: 7 [384000/394813 (97%)]\tLoss: 6.306544\n",
      "====> Epoch: 7 Average loss: 32.1881\n",
      "Train Epoch: 8 [0/394813 (0%)]\tLoss: 34.141502\n",
      "Train Epoch: 8 [12800/394813 (3%)]\tLoss: 44.225124\n",
      "Train Epoch: 8 [25600/394813 (6%)]\tLoss: 13.166590\n",
      "Train Epoch: 8 [38400/394813 (10%)]\tLoss: 28.435972\n",
      "Train Epoch: 8 [51200/394813 (13%)]\tLoss: 34.288185\n",
      "Train Epoch: 8 [64000/394813 (16%)]\tLoss: 30.141188\n",
      "Train Epoch: 8 [76800/394813 (19%)]\tLoss: 26.140369\n",
      "Train Epoch: 8 [89600/394813 (23%)]\tLoss: 41.078617\n",
      "Train Epoch: 8 [102400/394813 (26%)]\tLoss: 33.149654\n",
      "Train Epoch: 8 [115200/394813 (29%)]\tLoss: 25.399105\n",
      "Train Epoch: 8 [128000/394813 (32%)]\tLoss: 25.512432\n",
      "Train Epoch: 8 [140800/394813 (36%)]\tLoss: 33.179710\n",
      "Train Epoch: 8 [153600/394813 (39%)]\tLoss: 20.554115\n",
      "Train Epoch: 8 [166400/394813 (42%)]\tLoss: 36.492828\n",
      "Train Epoch: 8 [179200/394813 (45%)]\tLoss: 32.394043\n",
      "Train Epoch: 8 [192000/394813 (49%)]\tLoss: 41.843185\n",
      "Train Epoch: 8 [204800/394813 (52%)]\tLoss: 23.953403\n",
      "Train Epoch: 8 [217600/394813 (55%)]\tLoss: 44.741348\n",
      "Train Epoch: 8 [230400/394813 (58%)]\tLoss: 31.796799\n",
      "Train Epoch: 8 [243200/394813 (62%)]\tLoss: 54.548119\n",
      "Train Epoch: 8 [256000/394813 (65%)]\tLoss: 32.476509\n",
      "Train Epoch: 8 [268800/394813 (68%)]\tLoss: 37.120750\n",
      "Train Epoch: 8 [281600/394813 (71%)]\tLoss: 26.916492\n",
      "Train Epoch: 8 [294400/394813 (75%)]\tLoss: 46.323662\n",
      "Train Epoch: 8 [307200/394813 (78%)]\tLoss: 16.105494\n",
      "Train Epoch: 8 [320000/394813 (81%)]\tLoss: 39.550003\n",
      "Train Epoch: 8 [332800/394813 (84%)]\tLoss: 28.390694\n",
      "Train Epoch: 8 [345600/394813 (88%)]\tLoss: 36.924416\n",
      "Train Epoch: 8 [358400/394813 (91%)]\tLoss: 40.653473\n",
      "Train Epoch: 8 [371200/394813 (94%)]\tLoss: 36.023277\n",
      "Train Epoch: 8 [384000/394813 (97%)]\tLoss: 7.121614\n",
      "====> Epoch: 8 Average loss: 34.2807\n",
      "Train Epoch: 9 [0/394813 (0%)]\tLoss: 36.562180\n",
      "Train Epoch: 9 [12800/394813 (3%)]\tLoss: 46.727585\n",
      "Train Epoch: 9 [25600/394813 (6%)]\tLoss: 13.880873\n",
      "Train Epoch: 9 [38400/394813 (10%)]\tLoss: 30.502964\n",
      "Train Epoch: 9 [51200/394813 (13%)]\tLoss: 35.476883\n",
      "Train Epoch: 9 [64000/394813 (16%)]\tLoss: 32.058804\n",
      "Train Epoch: 9 [76800/394813 (19%)]\tLoss: 27.524082\n",
      "Train Epoch: 9 [89600/394813 (23%)]\tLoss: 43.824249\n",
      "Train Epoch: 9 [102400/394813 (26%)]\tLoss: 35.769062\n",
      "Train Epoch: 9 [115200/394813 (29%)]\tLoss: 25.934790\n",
      "Train Epoch: 9 [128000/394813 (32%)]\tLoss: 25.831436\n",
      "Train Epoch: 9 [140800/394813 (36%)]\tLoss: 35.405033\n",
      "Train Epoch: 9 [153600/394813 (39%)]\tLoss: 22.577492\n",
      "Train Epoch: 9 [166400/394813 (42%)]\tLoss: 38.311066\n",
      "Train Epoch: 9 [179200/394813 (45%)]\tLoss: 34.353722\n",
      "Train Epoch: 9 [192000/394813 (49%)]\tLoss: 44.320751\n",
      "Train Epoch: 9 [204800/394813 (52%)]\tLoss: 25.454792\n",
      "Train Epoch: 9 [217600/394813 (55%)]\tLoss: 47.022942\n",
      "Train Epoch: 9 [230400/394813 (58%)]\tLoss: 33.535885\n",
      "Train Epoch: 9 [243200/394813 (62%)]\tLoss: 57.169037\n",
      "Train Epoch: 9 [256000/394813 (65%)]\tLoss: 33.518414\n",
      "Train Epoch: 9 [268800/394813 (68%)]\tLoss: 38.844238\n",
      "Train Epoch: 9 [281600/394813 (71%)]\tLoss: 28.286194\n",
      "Train Epoch: 9 [294400/394813 (75%)]\tLoss: 48.374180\n",
      "Train Epoch: 9 [307200/394813 (78%)]\tLoss: 16.839607\n",
      "Train Epoch: 9 [320000/394813 (81%)]\tLoss: 42.777893\n",
      "Train Epoch: 9 [332800/394813 (84%)]\tLoss: 30.665997\n",
      "Train Epoch: 9 [345600/394813 (88%)]\tLoss: 39.600334\n",
      "Train Epoch: 9 [358400/394813 (91%)]\tLoss: 43.424980\n",
      "Train Epoch: 9 [371200/394813 (94%)]\tLoss: 37.715946\n",
      "Train Epoch: 9 [384000/394813 (97%)]\tLoss: 7.705728\n",
      "====> Epoch: 9 Average loss: 36.2142\n",
      "Train Epoch: 10 [0/394813 (0%)]\tLoss: 38.539879\n",
      "Train Epoch: 10 [12800/394813 (3%)]\tLoss: 49.597816\n",
      "Train Epoch: 10 [25600/394813 (6%)]\tLoss: 15.515114\n",
      "Train Epoch: 10 [38400/394813 (10%)]\tLoss: 33.655651\n",
      "Train Epoch: 10 [51200/394813 (13%)]\tLoss: 39.099167\n",
      "Train Epoch: 10 [64000/394813 (16%)]\tLoss: 33.418106\n",
      "Train Epoch: 10 [76800/394813 (19%)]\tLoss: 28.971619\n",
      "Train Epoch: 10 [89600/394813 (23%)]\tLoss: 45.201378\n",
      "Train Epoch: 10 [102400/394813 (26%)]\tLoss: 38.667988\n",
      "Train Epoch: 10 [115200/394813 (29%)]\tLoss: 27.602242\n",
      "Train Epoch: 10 [128000/394813 (32%)]\tLoss: 27.913391\n",
      "Train Epoch: 10 [140800/394813 (36%)]\tLoss: 37.943058\n",
      "Train Epoch: 10 [153600/394813 (39%)]\tLoss: 23.641270\n",
      "Train Epoch: 10 [166400/394813 (42%)]\tLoss: 40.114441\n",
      "Train Epoch: 10 [179200/394813 (45%)]\tLoss: 35.999569\n",
      "Train Epoch: 10 [192000/394813 (49%)]\tLoss: 47.057533\n",
      "Train Epoch: 10 [204800/394813 (52%)]\tLoss: 26.513977\n",
      "Train Epoch: 10 [217600/394813 (55%)]\tLoss: 48.636303\n",
      "Train Epoch: 10 [230400/394813 (58%)]\tLoss: 35.479790\n",
      "Train Epoch: 10 [243200/394813 (62%)]\tLoss: 59.061321\n",
      "Train Epoch: 10 [256000/394813 (65%)]\tLoss: 34.304207\n",
      "Train Epoch: 10 [268800/394813 (68%)]\tLoss: 40.869625\n",
      "Train Epoch: 10 [281600/394813 (71%)]\tLoss: 29.416973\n",
      "Train Epoch: 10 [294400/394813 (75%)]\tLoss: 51.029388\n",
      "Train Epoch: 10 [307200/394813 (78%)]\tLoss: 17.314384\n",
      "Train Epoch: 10 [320000/394813 (81%)]\tLoss: 43.850609\n",
      "Train Epoch: 10 [332800/394813 (84%)]\tLoss: 31.837957\n",
      "Train Epoch: 10 [345600/394813 (88%)]\tLoss: 41.530674\n",
      "Train Epoch: 10 [358400/394813 (91%)]\tLoss: 46.443874\n",
      "Train Epoch: 10 [371200/394813 (94%)]\tLoss: 40.330517\n",
      "Train Epoch: 10 [384000/394813 (97%)]\tLoss: 8.843962\n",
      "====> Epoch: 10 Average loss: 38.0100\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./model_realbook.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vérité\n",
      "['C:maj', 'C:maj', 'C:maj', 'C:maj', 'C:7', 'C:7', 'C:7', 'C:7', 'F:maj', 'F:maj', 'F:maj', 'F:maj', 'F:min7', 'F:min7', 'F:min7', 'F:min7']\n",
      "\n",
      "Par VAE\n",
      "['C:maj', 'C:maj', 'C:maj', 'C:maj', 'C:7', 'C:7', 'C:7', 'C:7', 'F:maj', 'F:maj', 'F:maj', 'F:maj', 'F:maj', 'F:min', 'F:min7', 'F:min7']\n"
     ]
    }
   ],
   "source": [
    "PITCH_LIST = [\"A\", \"A#\", \"B\", \"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\"]\n",
    "QUALITY_LIST = [\"maj\", \"min\", \"dim\", \"maj7\", \"min7\", \"7\", \"dim7\"]\n",
    "\n",
    "def sample_to_chords(sample):\n",
    "    idx_chords = np.argmax(sample[0,:,:],1)\n",
    "#     print(idx_chords)\n",
    "    chords = [PITCH_LIST[int(idx/7)] + \":\" + QUALITY_LIST[int(idx%7)] for idx in idx_chords]\n",
    "    return chords\n",
    "\n",
    "index_test = 16\n",
    "test_sample = realbook_dataset[0][index_test]\n",
    "\n",
    "print(\"Vérité\")\n",
    "true_sample = test_sample.view(1, 16, -1).numpy()\n",
    "print(sample_to_chords(true_sample))\n",
    "\n",
    "print()\n",
    "print(\"Par VAE\")\n",
    "model.to(torch.device(\"cpu\"))\n",
    "recons_test, _, _ = model(test_sample)\n",
    "print(sample_to_chords(recons_test.detach().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 16, 84)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['G:maj',\n",
       " 'G:maj',\n",
       " 'G:maj',\n",
       " 'A#:maj',\n",
       " 'G:maj',\n",
       " 'A#:maj',\n",
       " 'C:maj',\n",
       " 'A:min',\n",
       " 'A:min',\n",
       " 'A:min',\n",
       " 'A:min',\n",
       " 'A:7',\n",
       " 'A:7',\n",
       " 'D:min7',\n",
       " 'D:min7',\n",
       " 'D:min7']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_LATENT = 40\n",
    "with torch.no_grad():\n",
    "    sample = torch.randn(1, N_LATENT)\n",
    "    sample = model.decode(sample).cpu()\n",
    "    sample = sample.numpy()\n",
    "print(sample.shape)\n",
    "sample_to_chords(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3085"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
