{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import data_loader\n",
    "import numpy as np\n",
    "import sample_to_chords as s2c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda:0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():  \n",
    "  dev = \"cuda:0\" \n",
    "else:  \n",
    "  dev = \"cpu\"  \n",
    "print('using',dev)\n",
    "device = torch.device(dev)\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        N_HIDDEN = 400\n",
    "        N_LATENT = 40\n",
    "        \n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(16*12*7, N_HIDDEN)\n",
    "        self.fc21 = nn.Linear(N_HIDDEN, N_LATENT)\n",
    "        self.fc22 = nn.Linear(N_HIDDEN, N_LATENT)\n",
    "        self.fc3 = nn.Linear(N_LATENT, N_HIDDEN)\n",
    "        self.fc4 = nn.Linear(N_HIDDEN, 16*12*7)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        soft = nn.Sigmoid()\n",
    "        return soft(self.fc4(h3).view(-1, 16, 12*7))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 16*12*7))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar, beta):\n",
    "    BCE = F.binary_cross_entropy(recon_x.view(-1, 16*12*7), x.view(-1, 16*12*7), reduction='sum')\n",
    "\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + beta*KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    beta = epoch/epochs\n",
    "    for batch_idx, data in enumerate(realbook_dataset):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    \n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar, beta)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), Nchunks,\n",
    "                100. * batch_idx * len(data)/ Nchunks,\n",
    "                loss.item() / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / Nchunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 128\n",
    "log_interval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded !\n"
     ]
    }
   ],
   "source": [
    "realbook_dataset = data_loader.import_dataset()\n",
    "Nchunks = len(realbook_dataset)\n",
    "realbook_dataset = torch.split(realbook_dataset, batch_size, 0)\n",
    "model = VAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"./model_realbook.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/394813 (0%)]\tLoss: 940.960876\n",
      "Train Epoch: 1 [12800/394813 (3%)]\tLoss: 93.857872\n",
      "Train Epoch: 1 [25600/394813 (6%)]\tLoss: 35.944496\n",
      "Train Epoch: 1 [38400/394813 (10%)]\tLoss: 63.293659\n",
      "Train Epoch: 1 [51200/394813 (13%)]\tLoss: 62.157318\n",
      "Train Epoch: 1 [64000/394813 (16%)]\tLoss: 53.875652\n",
      "Train Epoch: 1 [76800/394813 (19%)]\tLoss: 41.695263\n",
      "Train Epoch: 1 [89600/394813 (23%)]\tLoss: 56.123219\n",
      "Train Epoch: 1 [102400/394813 (26%)]\tLoss: 45.890526\n",
      "Train Epoch: 1 [115200/394813 (29%)]\tLoss: 26.608919\n",
      "Train Epoch: 1 [128000/394813 (32%)]\tLoss: 24.649723\n",
      "Train Epoch: 1 [140800/394813 (36%)]\tLoss: 41.036995\n",
      "Train Epoch: 1 [153600/394813 (39%)]\tLoss: 20.808233\n",
      "Train Epoch: 1 [166400/394813 (42%)]\tLoss: 37.357555\n",
      "Train Epoch: 1 [179200/394813 (45%)]\tLoss: 28.931601\n",
      "Train Epoch: 1 [192000/394813 (49%)]\tLoss: 47.596252\n",
      "Train Epoch: 1 [204800/394813 (52%)]\tLoss: 17.650530\n",
      "Train Epoch: 1 [217600/394813 (55%)]\tLoss: 39.296452\n",
      "Train Epoch: 1 [230400/394813 (58%)]\tLoss: 23.975031\n",
      "Train Epoch: 1 [243200/394813 (62%)]\tLoss: 51.732967\n",
      "Train Epoch: 1 [256000/394813 (65%)]\tLoss: 26.379410\n",
      "Train Epoch: 1 [268800/394813 (68%)]\tLoss: 28.697765\n",
      "Train Epoch: 1 [281600/394813 (71%)]\tLoss: 19.333954\n",
      "Train Epoch: 1 [294400/394813 (75%)]\tLoss: 42.429119\n",
      "Train Epoch: 1 [307200/394813 (78%)]\tLoss: 7.247831\n",
      "Train Epoch: 1 [320000/394813 (81%)]\tLoss: 50.510918\n",
      "Train Epoch: 1 [332800/394813 (84%)]\tLoss: 16.721766\n",
      "Train Epoch: 1 [345600/394813 (88%)]\tLoss: 24.958250\n",
      "Train Epoch: 1 [358400/394813 (91%)]\tLoss: 30.440300\n",
      "Train Epoch: 1 [371200/394813 (94%)]\tLoss: 22.965702\n",
      "Train Epoch: 1 [384000/394813 (97%)]\tLoss: 2.858662\n",
      "====> Epoch: 1 Average loss: 40.8988\n",
      "Train Epoch: 2 [0/394813 (0%)]\tLoss: 28.817276\n",
      "Train Epoch: 2 [12800/394813 (3%)]\tLoss: 39.479374\n",
      "Train Epoch: 2 [25600/394813 (6%)]\tLoss: 7.279136\n",
      "Train Epoch: 2 [38400/394813 (10%)]\tLoss: 20.035706\n",
      "Train Epoch: 2 [51200/394813 (13%)]\tLoss: 24.473621\n",
      "Train Epoch: 2 [64000/394813 (16%)]\tLoss: 21.172447\n",
      "Train Epoch: 2 [76800/394813 (19%)]\tLoss: 14.926033\n",
      "Train Epoch: 2 [89600/394813 (23%)]\tLoss: 28.881626\n",
      "Train Epoch: 2 [102400/394813 (26%)]\tLoss: 21.225178\n",
      "Train Epoch: 2 [115200/394813 (29%)]\tLoss: 14.107866\n",
      "Train Epoch: 2 [128000/394813 (32%)]\tLoss: 13.734112\n",
      "Train Epoch: 2 [140800/394813 (36%)]\tLoss: 22.399708\n",
      "Train Epoch: 2 [153600/394813 (39%)]\tLoss: 11.794601\n",
      "Train Epoch: 2 [166400/394813 (42%)]\tLoss: 23.593620\n",
      "Train Epoch: 2 [179200/394813 (45%)]\tLoss: 18.471106\n",
      "Train Epoch: 2 [192000/394813 (49%)]\tLoss: 31.948261\n",
      "Train Epoch: 2 [204800/394813 (52%)]\tLoss: 14.959734\n",
      "Train Epoch: 2 [217600/394813 (55%)]\tLoss: 30.040600\n",
      "Train Epoch: 2 [230400/394813 (58%)]\tLoss: 19.842472\n",
      "Train Epoch: 2 [243200/394813 (62%)]\tLoss: 40.692345\n",
      "Train Epoch: 2 [256000/394813 (65%)]\tLoss: 18.830156\n",
      "Train Epoch: 2 [268800/394813 (68%)]\tLoss: 21.947693\n",
      "Train Epoch: 2 [281600/394813 (71%)]\tLoss: 16.006802\n",
      "Train Epoch: 2 [294400/394813 (75%)]\tLoss: 33.419559\n",
      "Train Epoch: 2 [307200/394813 (78%)]\tLoss: 7.876689\n",
      "Train Epoch: 2 [320000/394813 (81%)]\tLoss: 28.878819\n",
      "Train Epoch: 2 [332800/394813 (84%)]\tLoss: 14.967175\n",
      "Train Epoch: 2 [345600/394813 (88%)]\tLoss: 20.926483\n",
      "Train Epoch: 2 [358400/394813 (91%)]\tLoss: 26.939720\n",
      "Train Epoch: 2 [371200/394813 (94%)]\tLoss: 21.168159\n",
      "Train Epoch: 2 [384000/394813 (97%)]\tLoss: 3.554548\n",
      "====> Epoch: 2 Average loss: 22.7395\n",
      "Train Epoch: 3 [0/394813 (0%)]\tLoss: 25.032497\n",
      "Train Epoch: 3 [12800/394813 (3%)]\tLoss: 34.578491\n",
      "Train Epoch: 3 [25600/394813 (6%)]\tLoss: 8.214721\n",
      "Train Epoch: 3 [38400/394813 (10%)]\tLoss: 18.680447\n",
      "Train Epoch: 3 [51200/394813 (13%)]\tLoss: 23.932636\n",
      "Train Epoch: 3 [64000/394813 (16%)]\tLoss: 20.266853\n",
      "Train Epoch: 3 [76800/394813 (19%)]\tLoss: 15.944613\n",
      "Train Epoch: 3 [89600/394813 (23%)]\tLoss: 28.129288\n",
      "Train Epoch: 3 [102400/394813 (26%)]\tLoss: 21.340969\n",
      "Train Epoch: 3 [115200/394813 (29%)]\tLoss: 14.540903\n",
      "Train Epoch: 3 [128000/394813 (32%)]\tLoss: 14.514511\n",
      "Train Epoch: 3 [140800/394813 (36%)]\tLoss: 22.991970\n",
      "Train Epoch: 3 [153600/394813 (39%)]\tLoss: 12.572350\n",
      "Train Epoch: 3 [166400/394813 (42%)]\tLoss: 24.404259\n",
      "Train Epoch: 3 [179200/394813 (45%)]\tLoss: 19.735416\n",
      "Train Epoch: 3 [192000/394813 (49%)]\tLoss: 29.928520\n",
      "Train Epoch: 3 [204800/394813 (52%)]\tLoss: 16.411156\n",
      "Train Epoch: 3 [217600/394813 (55%)]\tLoss: 30.681477\n",
      "Train Epoch: 3 [230400/394813 (58%)]\tLoss: 20.824692\n",
      "Train Epoch: 3 [243200/394813 (62%)]\tLoss: 41.060345\n",
      "Train Epoch: 3 [256000/394813 (65%)]\tLoss: 20.929050\n",
      "Train Epoch: 3 [268800/394813 (68%)]\tLoss: 23.694485\n",
      "Train Epoch: 3 [281600/394813 (71%)]\tLoss: 17.414043\n",
      "Train Epoch: 3 [294400/394813 (75%)]\tLoss: 33.764149\n",
      "Train Epoch: 3 [307200/394813 (78%)]\tLoss: 9.508107\n",
      "Train Epoch: 3 [320000/394813 (81%)]\tLoss: 27.109003\n",
      "Train Epoch: 3 [332800/394813 (84%)]\tLoss: 17.074203\n",
      "Train Epoch: 3 [345600/394813 (88%)]\tLoss: 22.454348\n",
      "Train Epoch: 3 [358400/394813 (91%)]\tLoss: 29.138618\n",
      "Train Epoch: 3 [371200/394813 (94%)]\tLoss: 23.355663\n",
      "Train Epoch: 3 [384000/394813 (97%)]\tLoss: 4.044569\n",
      "====> Epoch: 3 Average loss: 23.1728\n",
      "Train Epoch: 4 [0/394813 (0%)]\tLoss: 26.765293\n",
      "Train Epoch: 4 [12800/394813 (3%)]\tLoss: 36.337627\n",
      "Train Epoch: 4 [25600/394813 (6%)]\tLoss: 9.123074\n",
      "Train Epoch: 4 [38400/394813 (10%)]\tLoss: 20.785740\n",
      "Train Epoch: 4 [51200/394813 (13%)]\tLoss: 25.612310\n",
      "Train Epoch: 4 [64000/394813 (16%)]\tLoss: 22.731415\n",
      "Train Epoch: 4 [76800/394813 (19%)]\tLoss: 17.569868\n",
      "Train Epoch: 4 [89600/394813 (23%)]\tLoss: 31.261127\n",
      "Train Epoch: 4 [102400/394813 (26%)]\tLoss: 23.524368\n",
      "Train Epoch: 4 [115200/394813 (29%)]\tLoss: 17.076744\n",
      "Train Epoch: 4 [128000/394813 (32%)]\tLoss: 16.258854\n",
      "Train Epoch: 4 [140800/394813 (36%)]\tLoss: 25.953661\n",
      "Train Epoch: 4 [153600/394813 (39%)]\tLoss: 14.270872\n",
      "Train Epoch: 4 [166400/394813 (42%)]\tLoss: 25.754398\n",
      "Train Epoch: 4 [179200/394813 (45%)]\tLoss: 22.042309\n",
      "Train Epoch: 4 [192000/394813 (49%)]\tLoss: 32.770855\n",
      "Train Epoch: 4 [204800/394813 (52%)]\tLoss: 18.409279\n",
      "Train Epoch: 4 [217600/394813 (55%)]\tLoss: 33.315952\n",
      "Train Epoch: 4 [230400/394813 (58%)]\tLoss: 22.362505\n",
      "Train Epoch: 4 [243200/394813 (62%)]\tLoss: 43.622719\n",
      "Train Epoch: 4 [256000/394813 (65%)]\tLoss: 23.453697\n",
      "Train Epoch: 4 [268800/394813 (68%)]\tLoss: 25.938362\n",
      "Train Epoch: 4 [281600/394813 (71%)]\tLoss: 20.151144\n",
      "Train Epoch: 4 [294400/394813 (75%)]\tLoss: 36.156639\n",
      "Train Epoch: 4 [307200/394813 (78%)]\tLoss: 10.688129\n",
      "Train Epoch: 4 [320000/394813 (81%)]\tLoss: 28.918041\n",
      "Train Epoch: 4 [332800/394813 (84%)]\tLoss: 19.336046\n",
      "Train Epoch: 4 [345600/394813 (88%)]\tLoss: 25.396301\n",
      "Train Epoch: 4 [358400/394813 (91%)]\tLoss: 31.548870\n",
      "Train Epoch: 4 [371200/394813 (94%)]\tLoss: 26.105936\n",
      "Train Epoch: 4 [384000/394813 (97%)]\tLoss: 4.732675\n",
      "====> Epoch: 4 Average loss: 25.2917\n",
      "Train Epoch: 5 [0/394813 (0%)]\tLoss: 28.921419\n",
      "Train Epoch: 5 [12800/394813 (3%)]\tLoss: 37.687130\n",
      "Train Epoch: 5 [25600/394813 (6%)]\tLoss: 10.577480\n",
      "Train Epoch: 5 [38400/394813 (10%)]\tLoss: 22.343075\n",
      "Train Epoch: 5 [51200/394813 (13%)]\tLoss: 28.175701\n",
      "Train Epoch: 5 [64000/394813 (16%)]\tLoss: 24.419884\n",
      "Train Epoch: 5 [76800/394813 (19%)]\tLoss: 19.914795\n",
      "Train Epoch: 5 [89600/394813 (23%)]\tLoss: 33.816887\n",
      "Train Epoch: 5 [102400/394813 (26%)]\tLoss: 26.838158\n",
      "Train Epoch: 5 [115200/394813 (29%)]\tLoss: 19.007425\n",
      "Train Epoch: 5 [128000/394813 (32%)]\tLoss: 19.302402\n",
      "Train Epoch: 5 [140800/394813 (36%)]\tLoss: 27.469860\n",
      "Train Epoch: 5 [153600/394813 (39%)]\tLoss: 15.509865\n",
      "Train Epoch: 5 [166400/394813 (42%)]\tLoss: 28.866142\n",
      "Train Epoch: 5 [179200/394813 (45%)]\tLoss: 24.763260\n",
      "Train Epoch: 5 [192000/394813 (49%)]\tLoss: 35.296432\n",
      "Train Epoch: 5 [204800/394813 (52%)]\tLoss: 19.779007\n",
      "Train Epoch: 5 [217600/394813 (55%)]\tLoss: 36.483341\n",
      "Train Epoch: 5 [230400/394813 (58%)]\tLoss: 25.298714\n",
      "Train Epoch: 5 [243200/394813 (62%)]\tLoss: 46.827324\n",
      "Train Epoch: 5 [256000/394813 (65%)]\tLoss: 24.584740\n",
      "Train Epoch: 5 [268800/394813 (68%)]\tLoss: 28.533630\n",
      "Train Epoch: 5 [281600/394813 (71%)]\tLoss: 21.795883\n",
      "Train Epoch: 5 [294400/394813 (75%)]\tLoss: 38.851208\n",
      "Train Epoch: 5 [307200/394813 (78%)]\tLoss: 12.278278\n",
      "Train Epoch: 5 [320000/394813 (81%)]\tLoss: 32.263672\n",
      "Train Epoch: 5 [332800/394813 (84%)]\tLoss: 22.444248\n",
      "Train Epoch: 5 [345600/394813 (88%)]\tLoss: 28.640926\n",
      "Train Epoch: 5 [358400/394813 (91%)]\tLoss: 34.617256\n",
      "Train Epoch: 5 [371200/394813 (94%)]\tLoss: 28.707428\n",
      "Train Epoch: 5 [384000/394813 (97%)]\tLoss: 5.323899\n",
      "====> Epoch: 5 Average loss: 27.6683\n",
      "Train Epoch: 6 [0/394813 (0%)]\tLoss: 31.123728\n",
      "Train Epoch: 6 [12800/394813 (3%)]\tLoss: 41.043491\n",
      "Train Epoch: 6 [25600/394813 (6%)]\tLoss: 11.353326\n",
      "Train Epoch: 6 [38400/394813 (10%)]\tLoss: 25.238628\n",
      "Train Epoch: 6 [51200/394813 (13%)]\tLoss: 30.741226\n",
      "Train Epoch: 6 [64000/394813 (16%)]\tLoss: 26.707069\n",
      "Train Epoch: 6 [76800/394813 (19%)]\tLoss: 22.138435\n",
      "Train Epoch: 6 [89600/394813 (23%)]\tLoss: 36.001160\n",
      "Train Epoch: 6 [102400/394813 (26%)]\tLoss: 28.542273\n",
      "Train Epoch: 6 [115200/394813 (29%)]\tLoss: 20.629360\n",
      "Train Epoch: 6 [128000/394813 (32%)]\tLoss: 21.299305\n",
      "Train Epoch: 6 [140800/394813 (36%)]\tLoss: 30.472527\n",
      "Train Epoch: 6 [153600/394813 (39%)]\tLoss: 17.174799\n",
      "Train Epoch: 6 [166400/394813 (42%)]\tLoss: 31.833763\n",
      "Train Epoch: 6 [179200/394813 (45%)]\tLoss: 27.434711\n",
      "Train Epoch: 6 [192000/394813 (49%)]\tLoss: 38.443588\n",
      "Train Epoch: 6 [204800/394813 (52%)]\tLoss: 21.678230\n",
      "Train Epoch: 6 [217600/394813 (55%)]\tLoss: 38.836227\n",
      "Train Epoch: 6 [230400/394813 (58%)]\tLoss: 27.803078\n",
      "Train Epoch: 6 [243200/394813 (62%)]\tLoss: 48.672195\n",
      "Train Epoch: 6 [256000/394813 (65%)]\tLoss: 27.145267\n",
      "Train Epoch: 6 [268800/394813 (68%)]\tLoss: 31.647766\n",
      "Train Epoch: 6 [281600/394813 (71%)]\tLoss: 23.082523\n",
      "Train Epoch: 6 [294400/394813 (75%)]\tLoss: 41.371704\n",
      "Train Epoch: 6 [307200/394813 (78%)]\tLoss: 13.416898\n",
      "Train Epoch: 6 [320000/394813 (81%)]\tLoss: 34.114136\n",
      "Train Epoch: 6 [332800/394813 (84%)]\tLoss: 24.927002\n",
      "Train Epoch: 6 [345600/394813 (88%)]\tLoss: 31.001406\n",
      "Train Epoch: 6 [358400/394813 (91%)]\tLoss: 36.757038\n",
      "Train Epoch: 6 [371200/394813 (94%)]\tLoss: 31.376705\n",
      "Train Epoch: 6 [384000/394813 (97%)]\tLoss: 5.594304\n",
      "====> Epoch: 6 Average loss: 29.9838\n",
      "Train Epoch: 7 [0/394813 (0%)]\tLoss: 33.572754\n",
      "Train Epoch: 7 [12800/394813 (3%)]\tLoss: 43.481514\n",
      "Train Epoch: 7 [25600/394813 (6%)]\tLoss: 12.686352\n",
      "Train Epoch: 7 [38400/394813 (10%)]\tLoss: 26.783489\n",
      "Train Epoch: 7 [51200/394813 (13%)]\tLoss: 32.749310\n",
      "Train Epoch: 7 [64000/394813 (16%)]\tLoss: 27.727753\n",
      "Train Epoch: 7 [76800/394813 (19%)]\tLoss: 24.268929\n",
      "Train Epoch: 7 [89600/394813 (23%)]\tLoss: 38.747841\n",
      "Train Epoch: 7 [102400/394813 (26%)]\tLoss: 32.254421\n",
      "Train Epoch: 7 [115200/394813 (29%)]\tLoss: 22.241413\n",
      "Train Epoch: 7 [128000/394813 (32%)]\tLoss: 23.125015\n",
      "Train Epoch: 7 [140800/394813 (36%)]\tLoss: 32.709591\n",
      "Train Epoch: 7 [153600/394813 (39%)]\tLoss: 18.972326\n",
      "Train Epoch: 7 [166400/394813 (42%)]\tLoss: 34.325577\n",
      "Train Epoch: 7 [179200/394813 (45%)]\tLoss: 29.937069\n",
      "Train Epoch: 7 [192000/394813 (49%)]\tLoss: 40.415115\n",
      "Train Epoch: 7 [204800/394813 (52%)]\tLoss: 23.133446\n",
      "Train Epoch: 7 [217600/394813 (55%)]\tLoss: 40.937397\n",
      "Train Epoch: 7 [230400/394813 (58%)]\tLoss: 28.435261\n",
      "Train Epoch: 7 [243200/394813 (62%)]\tLoss: 52.604809\n",
      "Train Epoch: 7 [256000/394813 (65%)]\tLoss: 29.151394\n",
      "Train Epoch: 7 [268800/394813 (68%)]\tLoss: 34.436516\n",
      "Train Epoch: 7 [281600/394813 (71%)]\tLoss: 25.107643\n",
      "Train Epoch: 7 [294400/394813 (75%)]\tLoss: 43.212753\n",
      "Train Epoch: 7 [307200/394813 (78%)]\tLoss: 14.942173\n",
      "Train Epoch: 7 [320000/394813 (81%)]\tLoss: 37.438934\n",
      "Train Epoch: 7 [332800/394813 (84%)]\tLoss: 27.407265\n",
      "Train Epoch: 7 [345600/394813 (88%)]\tLoss: 34.722698\n",
      "Train Epoch: 7 [358400/394813 (91%)]\tLoss: 39.483768\n",
      "Train Epoch: 7 [371200/394813 (94%)]\tLoss: 33.660370\n",
      "Train Epoch: 7 [384000/394813 (97%)]\tLoss: 5.881447\n",
      "====> Epoch: 7 Average loss: 32.2076\n",
      "Train Epoch: 8 [0/394813 (0%)]\tLoss: 35.337624\n",
      "Train Epoch: 8 [12800/394813 (3%)]\tLoss: 45.941597\n",
      "Train Epoch: 8 [25600/394813 (6%)]\tLoss: 13.775164\n",
      "Train Epoch: 8 [38400/394813 (10%)]\tLoss: 28.621502\n",
      "Train Epoch: 8 [51200/394813 (13%)]\tLoss: 35.539242\n",
      "Train Epoch: 8 [64000/394813 (16%)]\tLoss: 30.621702\n",
      "Train Epoch: 8 [76800/394813 (19%)]\tLoss: 25.706009\n",
      "Train Epoch: 8 [89600/394813 (23%)]\tLoss: 40.796585\n",
      "Train Epoch: 8 [102400/394813 (26%)]\tLoss: 33.572918\n",
      "Train Epoch: 8 [115200/394813 (29%)]\tLoss: 24.296841\n",
      "Train Epoch: 8 [128000/394813 (32%)]\tLoss: 23.947315\n",
      "Train Epoch: 8 [140800/394813 (36%)]\tLoss: 35.925247\n",
      "Train Epoch: 8 [153600/394813 (39%)]\tLoss: 20.113880\n",
      "Train Epoch: 8 [166400/394813 (42%)]\tLoss: 36.266754\n",
      "Train Epoch: 8 [179200/394813 (45%)]\tLoss: 31.845230\n",
      "Train Epoch: 8 [192000/394813 (49%)]\tLoss: 43.351112\n",
      "Train Epoch: 8 [204800/394813 (52%)]\tLoss: 24.353817\n",
      "Train Epoch: 8 [217600/394813 (55%)]\tLoss: 43.970261\n",
      "Train Epoch: 8 [230400/394813 (58%)]\tLoss: 30.791761\n",
      "Train Epoch: 8 [243200/394813 (62%)]\tLoss: 54.117420\n",
      "Train Epoch: 8 [256000/394813 (65%)]\tLoss: 32.150166\n",
      "Train Epoch: 8 [268800/394813 (68%)]\tLoss: 36.775097\n",
      "Train Epoch: 8 [281600/394813 (71%)]\tLoss: 27.879787\n",
      "Train Epoch: 8 [294400/394813 (75%)]\tLoss: 47.090343\n",
      "Train Epoch: 8 [307200/394813 (78%)]\tLoss: 16.057280\n",
      "Train Epoch: 8 [320000/394813 (81%)]\tLoss: 38.817085\n",
      "Train Epoch: 8 [332800/394813 (84%)]\tLoss: 27.651360\n",
      "Train Epoch: 8 [345600/394813 (88%)]\tLoss: 36.142788\n",
      "Train Epoch: 8 [358400/394813 (91%)]\tLoss: 41.194969\n",
      "Train Epoch: 8 [371200/394813 (94%)]\tLoss: 36.823631\n",
      "Train Epoch: 8 [384000/394813 (97%)]\tLoss: 6.776352\n",
      "====> Epoch: 8 Average loss: 34.2804\n",
      "Train Epoch: 9 [0/394813 (0%)]\tLoss: 38.603127\n",
      "Train Epoch: 9 [12800/394813 (3%)]\tLoss: 47.657852\n",
      "Train Epoch: 9 [25600/394813 (6%)]\tLoss: 14.328470\n",
      "Train Epoch: 9 [38400/394813 (10%)]\tLoss: 30.979034\n",
      "Train Epoch: 9 [51200/394813 (13%)]\tLoss: 36.552055\n",
      "Train Epoch: 9 [64000/394813 (16%)]\tLoss: 31.659100\n",
      "Train Epoch: 9 [76800/394813 (19%)]\tLoss: 27.753895\n",
      "Train Epoch: 9 [89600/394813 (23%)]\tLoss: 43.534233\n",
      "Train Epoch: 9 [102400/394813 (26%)]\tLoss: 36.037239\n",
      "Train Epoch: 9 [115200/394813 (29%)]\tLoss: 25.453709\n",
      "Train Epoch: 9 [128000/394813 (32%)]\tLoss: 26.148041\n",
      "Train Epoch: 9 [140800/394813 (36%)]\tLoss: 37.754143\n",
      "Train Epoch: 9 [153600/394813 (39%)]\tLoss: 21.677048\n",
      "Train Epoch: 9 [166400/394813 (42%)]\tLoss: 38.429451\n",
      "Train Epoch: 9 [179200/394813 (45%)]\tLoss: 33.545761\n",
      "Train Epoch: 9 [192000/394813 (49%)]\tLoss: 45.081680\n",
      "Train Epoch: 9 [204800/394813 (52%)]\tLoss: 26.400501\n",
      "Train Epoch: 9 [217600/394813 (55%)]\tLoss: 46.342934\n",
      "Train Epoch: 9 [230400/394813 (58%)]\tLoss: 32.495472\n",
      "Train Epoch: 9 [243200/394813 (62%)]\tLoss: 57.092495\n",
      "Train Epoch: 9 [256000/394813 (65%)]\tLoss: 33.754547\n",
      "Train Epoch: 9 [268800/394813 (68%)]\tLoss: 38.638176\n",
      "Train Epoch: 9 [281600/394813 (71%)]\tLoss: 29.310047\n",
      "Train Epoch: 9 [294400/394813 (75%)]\tLoss: 49.371529\n",
      "Train Epoch: 9 [307200/394813 (78%)]\tLoss: 17.331261\n",
      "Train Epoch: 9 [320000/394813 (81%)]\tLoss: 41.454102\n",
      "Train Epoch: 9 [332800/394813 (84%)]\tLoss: 31.153709\n",
      "Train Epoch: 9 [345600/394813 (88%)]\tLoss: 38.806854\n",
      "Train Epoch: 9 [358400/394813 (91%)]\tLoss: 43.549423\n",
      "Train Epoch: 9 [371200/394813 (94%)]\tLoss: 38.039574\n",
      "Train Epoch: 9 [384000/394813 (97%)]\tLoss: 6.939208\n",
      "====> Epoch: 9 Average loss: 36.2136\n",
      "Train Epoch: 10 [0/394813 (0%)]\tLoss: 39.067902\n",
      "Train Epoch: 10 [12800/394813 (3%)]\tLoss: 49.920700\n",
      "Train Epoch: 10 [25600/394813 (6%)]\tLoss: 14.859209\n",
      "Train Epoch: 10 [38400/394813 (10%)]\tLoss: 32.695663\n",
      "Train Epoch: 10 [51200/394813 (13%)]\tLoss: 39.423409\n",
      "Train Epoch: 10 [64000/394813 (16%)]\tLoss: 33.128700\n",
      "Train Epoch: 10 [76800/394813 (19%)]\tLoss: 29.539469\n",
      "Train Epoch: 10 [89600/394813 (23%)]\tLoss: 44.650433\n",
      "Train Epoch: 10 [102400/394813 (26%)]\tLoss: 38.816345\n",
      "Train Epoch: 10 [115200/394813 (29%)]\tLoss: 26.910282\n",
      "Train Epoch: 10 [128000/394813 (32%)]\tLoss: 27.041170\n",
      "Train Epoch: 10 [140800/394813 (36%)]\tLoss: 40.169781\n",
      "Train Epoch: 10 [153600/394813 (39%)]\tLoss: 23.199177\n",
      "Train Epoch: 10 [166400/394813 (42%)]\tLoss: 40.386021\n",
      "Train Epoch: 10 [179200/394813 (45%)]\tLoss: 35.376328\n",
      "Train Epoch: 10 [192000/394813 (49%)]\tLoss: 47.797447\n",
      "Train Epoch: 10 [204800/394813 (52%)]\tLoss: 27.655529\n",
      "Train Epoch: 10 [217600/394813 (55%)]\tLoss: 47.993183\n",
      "Train Epoch: 10 [230400/394813 (58%)]\tLoss: 34.371483\n",
      "Train Epoch: 10 [243200/394813 (62%)]\tLoss: 58.638569\n",
      "Train Epoch: 10 [256000/394813 (65%)]\tLoss: 34.844460\n",
      "Train Epoch: 10 [268800/394813 (68%)]\tLoss: 41.094246\n",
      "Train Epoch: 10 [281600/394813 (71%)]\tLoss: 30.615246\n",
      "Train Epoch: 10 [294400/394813 (75%)]\tLoss: 51.698502\n",
      "Train Epoch: 10 [307200/394813 (78%)]\tLoss: 18.823509\n",
      "Train Epoch: 10 [320000/394813 (81%)]\tLoss: 45.183311\n",
      "Train Epoch: 10 [332800/394813 (84%)]\tLoss: 31.960096\n",
      "Train Epoch: 10 [345600/394813 (88%)]\tLoss: 41.362274\n",
      "Train Epoch: 10 [358400/394813 (91%)]\tLoss: 46.954258\n",
      "Train Epoch: 10 [371200/394813 (94%)]\tLoss: 40.254951\n",
      "Train Epoch: 10 [384000/394813 (97%)]\tLoss: 7.471680\n",
      "====> Epoch: 10 Average loss: 38.0363\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./model_realbook.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vérité\n",
      "['C:maj', 'C:maj', 'C:maj', 'C:maj', 'C:7', 'C:7', 'C:7', 'C:7', 'F:maj', 'F:maj', 'F:maj', 'F:maj', 'F:min7', 'F:min7', 'F:min7', 'F:min7']\n",
      "\n",
      "Par VAE\n",
      "['C:maj', 'C:maj', 'C:maj', 'C:maj', 'C:7', 'C:7', 'C:7', 'C:7', 'F:maj', 'F:maj', 'F:maj', 'F:maj', 'F:min7', 'F:min7', 'F:min', 'C:maj']\n"
     ]
    }
   ],
   "source": [
    "PITCH_LIST = [\"A\", \"A#\", \"B\", \"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\"]\n",
    "QUALITY_LIST = [\"maj\", \"min\", \"dim\", \"maj7\", \"min7\", \"7\", \"dim7\"]\n",
    "\n",
    "def sample_to_chords(sample):\n",
    "    idx_chords = np.argmax(sample[0,:,:],1)\n",
    "#     print(idx_chords)\n",
    "    chords = [PITCH_LIST[int(idx/7)] + \":\" + QUALITY_LIST[int(idx%7)] for idx in idx_chords]\n",
    "    return chords\n",
    "\n",
    "index_test = 16\n",
    "test_sample = realbook_dataset[0][index_test]\n",
    "\n",
    "print(\"Vérité\")\n",
    "true_sample = test_sample.view(1, 16, -1).numpy()\n",
    "print(sample_to_chords(true_sample))\n",
    "\n",
    "print()\n",
    "print(\"Par VAE\")\n",
    "recons_test, _, _ = model(test_sample)\n",
    "print(sample_to_chords(recons_test.detach().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 16, 84)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['F:maj',\n",
       " 'F:maj',\n",
       " 'E:min7',\n",
       " 'E:min7',\n",
       " 'A:7',\n",
       " 'A:7',\n",
       " 'A:7',\n",
       " 'D:min7',\n",
       " 'D:min7',\n",
       " 'D:min7',\n",
       " 'D:min7',\n",
       " 'G:maj',\n",
       " 'G:maj',\n",
       " 'G:maj',\n",
       " 'F:maj',\n",
       " 'C:maj']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    sample = torch.randn(1, 40).to(device)\n",
    "    sample = model.decode(sample).cpu()\n",
    "    sample = sample.numpy()\n",
    "print(sample.shape)\n",
    "sample_to_chords(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3085"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
